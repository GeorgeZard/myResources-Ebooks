## 1.1 Ιστορικό και Κίνητρα  
  
Ιδιαίτερα σε δυναμικές και ασταθείς αγορές όπως οι αγορές bitcoin, η Βαθιά Ενισχυτική Μάθηση (DRL) έχει γίνει το κύριο παράδειγμα για τη δημιουργία αυτοματοποιημένων στρατηγικών διαπραγμάτευσης. Μέσω άμεσης αλληλεπίδρασης με την αγορά, οι πράκτορες DRL έχουν την ικανότητα να μαθαίνουν πολύπλοκες στρατηγικές συναλλαγών με ευκολία και να τροποποιούν τη συμπεριφορά τους ανάλογα με τις λαμβανόμενες ανταμοιβές για να μεγιστοποιούν τα οικονομικά αποτελέσματα.  
  
Αλλά τα εγγενή χαρακτηριστικά αυτών των αγορών—υψηλά επίπεδα θορύβου, γρήγορες αλλαγές στη δυναμική (μη-σταθερότητα) και γενική πολυπλοκότητα—συγκεντρώνουν σοβαρά την επιτυχία της εφαρμογής του DRL στο χρηματοοικονομικό εμπόριο. Αυτά τα στοιχεία συχνά προκαλούν σημαντική αστάθεια κατά τη διάρκεια της διαδικασίας εκπαίδευσης του DRL, η οποία παράγει πολιτικές που μπορεί να δείχνουν ασυνεπή απόδοση και να λείπουν ανθεκτικότητα όταν χρησιμοποιούνται σε πραγματικά περιβάλλοντα συναλλαγών. Ένα σημαντικό εμπόδιο για τη γενική αποδοχή της DRL για αυτοματοποιημένο trading είναι η αναξιοπιστία που προκύπτει από αυτή την αστάθεια στην εκπαίδευση.  
  
Η αντιμετώπιση αυτού του σημαντικού προβλήματος απαιτεί την έρευνα και εφαρμογή στρατηγικών που προορίζονται ειδικά για τη βελτίωση της σταθερότητας και της απόδοσης των πρακτόρων DRL που εργάζονται σε θορυβώδη χρηματοοικονομικά περιβάλλοντα. Η διερεύνηση της **Εξαγωγής Γνώσης (KD)** ως κύριας μεθόδου για τη μείωση της αστάθειας εκπαίδευσης του DRL και την ενίσχυση της απόδοσης στο χρηματοοικονομικό εμπόριο είναι ο κύριος στόχος αυτής της διατριβής. Η KD είναι βασικά η διαδικασία μεταφοράς γνώσης από ένα ή περισσότερα μοντέλα "δασκάλων" σε ένα μοντέλο "μαθητή", προσφέροντας έτσι ένα δομημένο σήμα μάθησης που μπορεί να βοηθήσει στη σταθεροποίηση της εκπαίδευσης του μαθητή.  
  
Εκτενής σε προχωρημένες ιδέες σε αυτόν τον τομέα, αυτή η μελέτη εξετάζει συγκεκριμένες τεχνικές KD που προορίζονται για DRL στις χρηματοπιστωτικές αγορές. Αυτά περιλαμβάνουν τη χρήση **συνόλων δασκάλων** για την παροχή πλουσιότερης και ίσως πιο ισχυρής καθοδήγησης, τη χρήση τεχνικών **πιθανοτικής μεταφοράς γνώσης** που επικεντρώνονται στην απόσταξη πληροφοριών από ενδιάμεσα επίπεδα του δικτύου μέσω της αντιστοίχισης των κατανομών χαρακτηριστικών αντί μόνο των τελικών εξόδων, και τη χρήση διαδικασιών **διαδικτυακής απόσταξης** όπου ο δάσκαλος και ο μαθητής μαθαίνουν ταυτόχρονα. Εξετασμένα σε σχετικά ερευνητικά περιβάλλοντα, αυτά τα προηγμένα παραλλαγμένα KD παρουσιάζουν ενδιαφέροντες δρόμους για την ενίσχυση της δυναμικής εκπαίδευσης του DRL.  
  
Έτσι, η κύρια κινητήρια δύναμη πίσω από αυτή τη διατριβή είναι η αυστηρή εφαρμογή και αξιολόγηση αυτών των προηγμένων τεχνικών Απόσταξης Γνώσης μέσα σε ένα πλαίσιο Βαθιάς Ενισχυτικής Μάθησης, όπως αυτό που εφαρμόζει τον αλγόριθμο Βελτιστοποίησης Πολιτικής Εγγύτητας (PPO). Βασιζόμενοι μόνο σε χαρακτηριστικά τιμών που έχουν σχεδιαστεί από ιστορικά δεδομένα OHLCV, όπως ορίζονται από το σύνολο δεδομένων και τη μεθοδολογία μηχανικής χαρακτηριστικών που περιγράφεται σε σχετική προηγούμενη εργασία (π.χ., Moustakidis 4.1.1 & 4.1.2), ο στόχος είναι να βελτιωθεί σαφώς τόσο η σταθερότητα όσο και η κερδοφορία των πρακτόρων DRL, ειδικά για το εμπόριο κρυπτονομισμάτων. Αυτή η μελέτη στοχεύει στην παροχή εμπειρικών δεδομένων που αποδεικνύουν ότι οι προσαρμοσμένες τεχνικές KD μπορούν να επιλύσουν αποτελεσματικά τα κοινά προβλήματα αστάθειας στο χρηματοοικονομικό DRL, παράγοντας έτσι πιο αξιόπιστα και επιτυχημένα αυτοματοποιημένα συστήματα διαπραγμάτευσης. 
## 1.2 Δήλωση Προβλήματος: Προχωρημένη Απόσταξη Γνώσης για Σταθερό Trading DRL στις Αγορές Κρυπτονομισμάτων  
  
Βασισμένη στην επιθυμία να επιλυθούν οι εγγενείς αστάθειες εκπαίδευσης και οι περιορισμοί απόδοσης της Βαθιάς Ενισχυτικής Μάθησης (DRL) σε μεταβαλλόμενα χρηματοοικονομικά περιβάλλοντα, ειδικά στο εμπόριο bitcoin, αυτή η διατριβή καθορίζει ένα συγκεκριμένο ερευνητικό ζήτημα. Διατυπωμένο ως εξής είναι το κεντρικό ερευνητικό ζήτημα:  
  
Πώς μπορούν οι προηγμένες μέθοδοι Απόσταξης Γνώσης (KD)—ιδιαίτερα, η πιθανοτική μεταφορά γνώσης από ενδιάμεσα στρώματα και οι διαδικτυακές τεχνικές συνόλου—να ενσωματωθούν επιτυχώς σε ένα πλαίσιο Βαθιάς Ενισχυτικής Μάθησης (π.χ., Βελτιστοποίηση Πολιτικής Εγγύτητας - PPO) για να αντιμετωπιστεί η αστάθεια εκπαίδευσης και να βελτιωθεί σημαντικά η απόδοση των συναλλαγών (π.χ., Κέρδος και Ζημία) των πρακτόρων σε ασταθείς αγορές κρυπτονομισμάτων χρησιμοποιώντας μόνο μηχανικά χαρακτηριστικά βασισμένα σε τιμές;  
  
Η επιτυχής αντιμετώπιση αυτού του ζητήματος απαιτεί την επίλυση αρκετών σημαντικών συνδεδεμένων υπο-προβλημάτων και τεχνικών προκλήσεων:  
  
Ο καθορισμός της ακριβούς προσέγγισης για την ενσωμάτωση τόσο της πιθανοθεωρητικής KD, η οποία περιλαμβάνει την αντιστοίχιση των κατανομών χαρακτηριστικών από ενδιάμεσα στρώματα του δικτύου, όσο και πιθανώς της τυπικής αντιστοίχισης λογιστικών με τον βασικό στόχο βελτιστοποίησης του DRL, όπως η συνάρτηση απώλειας PPO, αποτελεί μια σημαντική πρόκληση. Αυτό εξαρτάται από την προσεκτική διατύπωση και την εξισορρόπηση των επιμέρους συναρτήσεων απώλειας για να διασφαλιστεί η αποτελεσματική μεταφορά γνώσης χωρίς να παρεμβαίνει στη διαδικασία ενισχυτικής μάθησης.  
Η ανάπτυξη ισχυρών σχεδίων για τον έλεγχο του συνόλου των δασκάλων σε ένα διαδικτυακό περιβάλλον είναι απολύτως ζωτικής σημασίας. Για να εξασφαλιστεί η σταθερή και ευνοϊκή μεταφορά γνώσης στον μαθητή πράκτορα σε ένα μη σταθερό περιβάλλον, αυτό περιλαμβάνει την επιλογή αποδοτικών προσεγγίσεων για την επιλογή ή την αξιολόγηση των δασκάλων και τη δυναμική συγκέντρωση γνώσης από πολλούς δασκάλους κατά τη διάρκεια της διαδικασίας εκπαίδευσης.  
Σε σύγκριση με τις βασικές μεθόδους που δεν χρησιμοποιούν αυτές τις τεχνικές, είναι επιτακτικό να αποδειχθεί εμπειρικά, μέσω αυστηρών πειραμάτων που περιλαμβάνουν αρκετές εκπαιδευτικές δοκιμές με διαφορετικούς τυχαίους σπόρους, ότι οι προτεινόμενες προηγμένες μέθοδοι KD μειώνουν αποτελεσματικά την παραλλακτικότητα και βελτιώνουν τη σταθερότητα σύγκλισης της διαδικασίας εκπαίδευσης DRL.  
Χρησιμοποιώντας αναδρομική δοκιμή σε αόρατα ιστορικά δεδομένα, η έρευνα πρέπει να επικυρώσει εμπειρικά, με στατιστικά σημαντική βελτίωση των μετρικών απόδοσης συναλλαγών, όπως το σωρευτικό Κέρδος και Ζημία (PnL), μεταξύ των πρακτόρων που εκπαιδεύτηκαν με τις προτεινόμενες προηγμένες τεχνικές KD και εκείνων που εκπαιδεύτηκαν χωρίς KD ή με απλότερες, λιγότερο προηγμένες μεθόδους KD.  
  
Αυτή η εργασία ουσιαστικά ασχολείται με την ιδιαίτερη εφαρμογή και την εμπειρική επικύρωση αρκετών προηγμένων μεθόδων απόσταξης γνώσης. Ο στόχος είναι να δείξουμε την ικανότητά τους να ξεπερνούν γνωστούς περιορισμούς του DRL, ειδικά την αστάθεια εκπαίδευσης, στο απαιτητικό περιβάλλον του αυτοματοποιημένου εμπορίου bitcoin χρησιμοποιώντας μόνο μηχανικά χαρακτηριστικά βασισμένα σε τιμές.
  
## 1.3 Ερευνητικά Ερωτήματα  
  
Βασισμένη στην επιθυμία να επιλυθούν οι εγγενείς αστάθειες εκπαίδευσης και οι περιορισμοί απόδοσης της Βαθιάς Ενισχυτικής Μάθησης (DRL) σε μεταβαλλόμενα χρηματοοικονομικά περιβάλλοντα, ειδικά στο εμπόριο bitcoin, αυτή η διατριβή καθορίζει ένα συγκεκριμένο ερευνητικό ζήτημα. Διατυπωμένο ως εξής είναι το κεντρικό ερευνητικό ζήτημα:  
  
Πώς μπορούν οι προηγμένες μέθοδοι Απόσταξης Γνώσης (KD)—ιδιαίτερα, η πιθανοτική μεταφορά γνώσης από ενδιάμεσα στρώματα και οι διαδικτυακές τεχνικές συνόλου—να ενσωματωθούν επιτυχώς σε ένα πλαίσιο Βαθιάς Ενισχυτικής Μάθησης (π.χ., Βελτιστοποίηση Πολιτικής Εγγύτητας - PPO) για να αντιμετωπιστεί η αστάθεια εκπαίδευσης και να βελτιωθεί σημαντικά η απόδοση των συναλλαγών (π.χ., Κέρδος και Ζημία) των πρακτόρων σε ασταθείς αγορές κρυπτονομισμάτων χρησιμοποιώντας μόνο μηχανικά χαρακτηριστικά βασισμένα σε τιμές;  
  
Η επιτυχής αντιμετώπιση αυτού του ζητήματος απαιτεί την επίλυση αρκετών σημαντικών συνδεδεμένων υπο-προβλημάτων και τεχνικών προκλήσεων:  
  
Ο καθορισμός της ακριβούς προσέγγισης για την ενσωμάτωση τόσο της πιθανοθεωρητικής KD, η οποία περιλαμβάνει την αντιστοίχιση των κατανομών χαρακτηριστικών από ενδιάμεσα στρώματα του δικτύου, όσο και πιθανώς της τυπικής αντιστοίχισης λογιστικών με τον βασικό στόχο βελτιστοποίησης του DRL, όπως η συνάρτηση απώλειας PPO, αποτελεί μια σημαντική πρόκληση. Αυτό εξαρτάται από την προσεκτική διατύπωση και την εξισορρόπηση των επιμέρους συναρτήσεων απώλειας για να διασφαλιστεί η αποτελεσματική μεταφορά γνώσης χωρίς να παρεμβαίνει στη διαδικασία ενισχυτικής μάθησης.  
Η ανάπτυξη ισχυρών σχεδίων για τον έλεγχο του συνόλου των δασκάλων σε ένα διαδικτυακό περιβάλλον είναι απολύτως ζωτικής σημασίας. Για να εξασφαλιστεί η σταθερή και ευνοϊκή μεταφορά γνώσης στον μαθητή πράκτορα σε ένα μη σταθερό περιβάλλον, αυτό περιλαμβάνει την επιλογή αποδοτικών προσεγγίσεων για την επιλογή ή την αξιολόγηση των δασκάλων και τη δυναμική συγκέντρωση γνώσης από πολλούς δασκάλους κατά τη διάρκεια της διαδικασίας εκπαίδευσης.  
Σε σύγκριση με τις βασικές μεθόδους που δεν χρησιμοποιούν αυτές τις τεχνικές, είναι επιτακτικό να αποδειχθεί εμπειρικά, μέσω αυστηρών πειραμάτων που περιλαμβάνουν αρκετές εκπαιδευτικές δοκιμές με διαφορετικούς τυχαίους σπόρους, ότι οι προτεινόμενες προηγμένες μέθοδοι KD μειώνουν αποτελεσματικά την παραλλακτικότητα και βελτιώνουν τη σταθερότητα σύγκλισης της διαδικασίας εκπαίδευσης DRL.  
Χρησιμοποιώντας αναδρομική δοκιμή σε αόρατα ιστορικά δεδομένα, η έρευνα πρέπει να επικυρώσει εμπειρικά, με στατιστικά σημαντική βελτίωση των μετρικών απόδοσης συναλλαγών, όπως το σωρευτικό Κέρδος και Ζημία (PnL), μεταξύ των πρακτόρων που εκπαιδεύτηκαν με τις προτεινόμενες προηγμένες τεχνικές KD και εκείνων που εκπαιδεύτηκαν χωρίς KD ή με απλότερες, λιγότερο προηγμένες μεθόδους KD.  
  
Αυτή η εργασία ουσιαστικά ασχολείται με την ιδιαίτερη εφαρμογή και την εμπειρική επικύρωση αρκετών προηγμένων μεθόδων απόσταξης γνώσης. Ο στόχος είναι να δείξουμε την ικανότητά τους να ξεπερνούν γνωστούς περιορισμούς του DRL, ειδικά την αστάθεια εκπαίδευσης, στο απαιτητικό περιβάλλον του αυτοματοποιημένου εμπορίου bitcoin χρησιμοποιώντας μόνο μηχανικά χαρακτηριστικά βασισμένα σε τιμές.
## 1.4 Σχέδιο Διατριβής  
  
Έξι κεφάλαια αποτελούν αυτή τη διατριβή, παρουσιάζοντας μεθοδικά τις μελέτες σχετικά με τη χρήση προηγμένων τεχνικών Απόσταξης Γνώσης στη Βαθιά Ενισχυτική Μάθηση για το εμπόριο bitcoin. Τα κεφάλαια είναι οργανωμένα λογικά από την έρευνα και τον ορισμό του προβλήματος μέχρι την τεχνική προσέγγιση, την πειραματική αξιολόγηση και τα τελικά σχόλια.  
  
* **Κεφάλαιο 1: Εισαγωγή:** Αυτό το κεφάλαιο παρουσιάζει τον ερευνητικό τομέα, τονίζοντας τη δυνατή χρήση της Βαθιάς Ενισχυτικής Μάθησης (DRL) για αυτοματοποιημένο χρηματοοικονομικό trading, αλλά υπογραμμίζοντας τη σημαντική δυσκολία της αστάθειας κατά την εκπαίδευση σε ασταθείς αγορές όπως τα κρυπτονομίσματα. Οδηγεί στην έρευνα της Απόσταξης Γνώσης (KD) ως λύση για αυτά τα προβλήματα. Επίσημα, το κεφάλαιο παρουσιάζει τη δήλωση του προβλήματος, διευκρινίζει τις συγκεκριμένες ερευνητικές ερωτήσεις δίνοντας έμφαση στην εφαρμογή της Πιθανοτικής Μετάδοσης Γνώσης (PKT) και της διαδικτυακής συνδυαστικής KD, και προσφέρει αυτή τη δομή της διατριβής.  
* **Κεφάλαιο 2: Σχετική Εργασία και Υπόβαθρο:** Οι απαραίτητες θεωρητικές βάσεις για τη μελέτη τίθενται σε αυτό το κεφάλαιο. Με γνώμονα τις ειδικές ποιότητες του εμπορίου bitcoin, προσφέρει υπόβαθρο για τις χρηματοπιστωτικές αγορές. Ασχολείται με τις σχετικές αρχιτεκτονικές Deep Learning, συμπεριλαμβανομένων των LSTMs, και καλύπτει τα θεμέλια της Deep Reinforcement Learning, συμπεριλαμβανομένου του αλγορίθμου Proximal Policy Optimisation (PPO). Το κεφάλαιο επίσης παρουσιάζει τις θεμελιώδεις ιδέες της Απόσταξης Γνώσης. Μετά το υπόβαθρο, εξετάζει σχετική έρευνα για τη χρήση του DRL στο χρηματοοικονομικό εμπόριο και την τρέχουσα εργασία πάνω στην Απόσταξη Γνώσης, επισημαίνοντας το συγκεκριμένο κενό που επιδιώκουν να καλύψουν οι προτεινόμενες προηγμένες τεχνικές KD.  
*  **Κεφάλαιο 3: Προτεινόμενη Προσέγγιση:** Η τεχνική προσέγγιση που εξελίχθηκε σε αυτή τη διατριβή καλύπτεται σε αυτό το κεφάλαιο. Βασισμένο στον αλγόριθμο PPO, παρουσιάζει το προτεινόμενο πλαίσιο DRL και συζητά την ενσωμάτωση των προηγμένων τεχνικών Απόσταξης Γνώσης. Αυτό καλύπτει μια λεπτομερή επισκόπηση της Πιθανοτικής Μετάδοσης Γνώσης (PKT) από ενδιάμεσα επίπεδα δικτύου και της συγκεκριμένης τεχνικής διαδικτυακής συνδυαστικής απόσταξης που χρησιμοποιείται εδώ. Λεπτομέρειες σχετικά με την αρχιτεκτονική του χρησιμοποιούμενου νευρωνικού δικτύου και τη διαμόρφωση της συνδυασμένης συνάρτησης απώλειας για την εκπαίδευση δίνονται σε αυτό το κεφάλαιο.  
*  **Κεφάλαιο 4: Πειραματική Ανασκόπηση** Η λεπτομερής διαμόρφωση για την εμπειρική αξιολόγηση της προτεινόμενης προσέγγισης καλύπτεται σε αυτό το κεφάλαιο. Αναφερόμενο στη μεθοδολογία από σχετική προηγούμενη εργασία (π.χ., Moustakidis 4.1.1 & 4.1.2), περιγράφει το συγκεκριμένο σύνολο δεδομένων κρυπτονομισμάτων που χρησιμοποιήθηκε για τη μελέτη, συμπεριλαμβανομένων των μηχανικών χαρακτηριστικών που βασίζονται σε τιμές και προέρχονται από δεδομένα OHLCV. Ορίζονται οι βασικές τεχνικές που χρησιμοποιούνται για σύγκριση—εκπαίδευση χωρίς KD και εφαρμογή τυπικών μεθόδων KD. Τονίζοντας την απόδοση των συναλλαγών (π.χ., PnL) και μετρήσιμα μέτρα σταθερότητας εκπαίδευσης σε αρκετές ανεξάρτητες εκτελέσεις, το κεφάλαιο αναλύει τα μετρικά αξιολόγησης που χρησιμοποιήθηκαν. Επιπλέον, παρέχονται λεπτομέρειες για την υλοποίηση.  
*  **Κεφάλαιο 5: Συζήτηση και Αποτελέσματα:** Τα εμπειρικά αποτελέσματα των πειραμάτων που πραγματοποιήθηκαν παρουσιάζονται σε αυτό το κεφάλαιο. Τα εμπειρικά αποτελέσματα των διεξαχθέντων πειραμάτων παρουσιάζονται σε αυτό το κεφάλαιο. Αναλύοντας τόσο τη σταθερότητα εκπαίδευσης όσο και την απόδοση συναλλαγών (PnL) σε μη ορατά δεδομένα αγοράς bitcoin, περιλαμβάνει μια ποσοτική σύγκριση των προτεινόμενων προηγμένων προσεγγίσεων KD σε σχέση με τις καθορισμένες βάσεις αναφοράς. Το κεφάλαιο προσφέρει μια ολοκληρωμένη μελέτη και ερμηνεία αυτών των ευρημάτων μαζί με μια συζήτηση για τη σημασία τους και μια σύνδεση μεταξύ τους και των ζητημάτων που τέθηκαν στο Κεφάλαιο 1.  
* **Κεφάλαιο 6: Συμπέρασμα:** Ολόκληρη η ερευνητική προσπάθεια συγκεντρώνεται σε αυτό το τελευταίο κεφάλαιο. Υπογραμμίζει τα κύρια αποτελέσματα σχετικά με την αποτελεσματικότητα των προτεινόμενων προηγμένων τεχνικών Απόσταξης Γνώσης για τη βελτίωση των πρακτόρων συναλλαγών κρυπτονομισμάτων που βασίζονται σε DRL. Το κεφάλαιο ανασκοπεί τα κύρια σημεία της διατριβής, προσφέρει συνοπτικές απαντήσεις στις ερευνητικές ερωτήσεις, αντιμετωπίζει τυχόν περιορισμούς της παρούσας εργασίας και παραθέτει ενδιαφέρουσες μελλοντικές κατευθύνσεις για αυτόν τον τομέα έρευνας.
## Κεφάλαιο 2: Υπόβαθρο και Σχετική Έρευνα Κεφάλαιο 2: Ιστορικό και Σχετική Εργασία  
  
Αυτό το κεφάλαιο στοχεύει να παρουσιάσει μια ανασκόπηση των σχετικών προηγούμενων μελετών που υποστηρίζουν τη μεθοδολογία και τις συνεισφορές αυτής της διατριβής, καθώς και να καθορίσει το βασικό θεωρητικό υπόβαθρο. Για να εκτιμήσει πλήρως την πολυπλοκότητα της χρήσης προηγμένων τεχνικών Απόσταξης Γνώσης (KD) στη Βαθιά Ενισχυτική Μάθηση (DRL) στο συγκεκριμένο πλαίσιο του εμπορίου bitcoin, πρέπει να έχει μια ισχυρή επίγνωση αυτών των θεμελιωδών ιδεών.  
  
Αυτό θα επιτρέψει στα επόμενα μέρη αυτού του κεφαλαίου να ασχοληθούν με αρκετούς σημαντικούς τομείς:  
* Βασικές ιδέες Μηχανικής Μάθησης, παρέχοντας μια ευρεία περίληψη των κύριων παραδειγμάτων στην επιστήμη αυτή.  
* Σχετικές Αρχιτεκτονικές Τεχνητών Νευρωνικών Δικτύων και Βαθιάς Μάθησης, όπως τα LSTM, που είναι σημαντικές για την επεξεργασία των δεδομένων που χρησιμοποιούνται σε αυτήν την έρευνα·  
* Οι αρχές της Ενισχυτικής Μάθησης και της Βαθιάς Ενισχυτικής Μάθησης, με λεπτομέρειες για αλγορίθμους όπως ο PPO, οι οποίοι αποτελούν τη βάση για το σχεδιασμό του πράκτορα συναλλαγών.  
Εξετάζονται σε αυτή τη μελέτη βασικές ιδέες που σχετίζονται με τις χρηματοπιστωτικές αγορές και τα ειδικά χαρακτηριστικά του εμπορίου κρυπτονομισμάτων, μαζί με μεθόδους απόσταξης γνώσης που εξηγούν τόσο τις τυπικές προσεγγίσεις KD όσο και τις προηγμένες τεχνικές που περιλαμβάνουν πιθανοθεωρητική μεταφορά γνώσης και διαδικτυακά σύνολα.  
  
Η ανασκόπηση σχετικής εργασίας στους τομείς της DRL για τα οικονομικά και της Απόσταξης Γνώσης θα ολοκληρώσει το κεφάλαιο και θα επισημάνει ιδιαίτερα τις νέες συνεισφορές αυτής της διατριβής, επισημαίνοντας τα κενά στην τρέχουσα έρευνα που αυτή η εργασία προσπαθεί να καλύψει.
### 2.1 Θεμελιώδη Στοιχεία Μηχανικής Μάθησης  
  
Στην τεχνητή νοημοσύνη, η μηχανική μάθηση (ML) είναι ένας υποτομέας που επικεντρώνεται στη δημιουργία αλγορίθμων που επιτρέπουν στους υπολογιστές να μαθαίνουν από δεδομένα χωρίς ρητή προγραμματισμένη παρέμβαση. Οι αλγόριθμοι θα πρέπει, ουσιαστικά, να βρίσκουν τάσεις, να δημιουργούν προβλέψεις ή να βασίζουν τις αποφάσεις τους στα δεδομένα εισόδου. Τρεις κύριες κατηγορίες ορίζουν τα παραδείγματα μηχανικής μάθησης ανάλογα με τον τύπο των δεδομένων και το διαθέσιμο σήμα μάθησης:  
  
* **Επιβλεπόμενη Μάθηση:** Κάτω από αυτό το παράδειγμα, κάθε σημείο δεδομένων αντιστοιχεί σε μια ετικέτα εξόδου ή τιμή στόχου, έτσι ώστε να εκπαιδεύονται μοντέλα σε ετικετοποιημένα σύνολα δεδομένων. Η μέθοδος μαθαίνει μια συνάρτηση αντιστοίχισης ετικετών εξόδου προς χαρακτηριστικά εισόδου. Τυπικές εργασίες περιλαμβάνουν **παλινδρόμηση**, η οποία προβλέπει συνεχείς αριθμητικές τιμές (π.χ., τιμές μετοχών), και **κατηγοριοποίηση**, η οποία προβλέπει διακριτές κατηγορίες (π.χ., συναίσθημα ως θετικό ή αρνητικό).  
*  **Η μη επιβλεπόμενη μάθηση** αναφέρεται σε Αυτό το παράδειγμα ασχολείται με μη επισημαδεμένα δεδομένα, σε αντίθεση με την επιβλεπόμενη μάθηση. Στη μη εποπτευόμενη μάθηση, οι αλγόριθμοι αναζητούν στα δεδομένα φυσικά μοτίβα, δομές ή σχέσεις χωρίς σαφή καθοδήγηση. Τυπικές εργασίες περιλαμβάνουν την **εξόρυξη κανόνων συσχέτισης**, η οποία βρίσκει σχέσεις μεταξύ μεταβλητών (π.χ., ταυτόχρονα εμφανιζόμενα αντικείμενα σε ένα σύνολο δεδομένων), και τον **ομαδοποιημένο**, ο οποίος ομαδοποιεί τα σημεία δεδομένων με βάση την ομοιότητα (π.χ., τμηματοποίηση πελατών).  
* **Ενισχυτική Μάθηση (RL):** Αυτό το παράδειγμα έχει έναν πράκτορα που αλληλεπιδρά με ένα περιβάλλον μαθαίνοντας να παίρνει μια σειρά από αποφάσεις. Βασισμένο στις δραστηριότητές του, ο πράκτορας λαμβάνει είτε ανταμοιβές είτε ποινές· ο στόχος του είναι να μάθει μια πολιτική που μεγιστοποιεί τη συνολική ανταμοιβή με την πάροδο του χρόνου. Τα προβλήματα που απαιτούν διαδοχική λήψη αποφάσεων υπό αβεβαιότητα απαιτούν ιδιαίτερα την Ενισχυτική Μάθηση (RL). Αυτή η διατριβή αναπτύσσει κυρίως στρατηγικές αλγοριθμικού trading χρησιμοποιώντας το παραδειγμα Reinforcement Learning, και πιο συγκεκριμένα την βαθιά του παραλλαγή, το Deep Reinforcement Learning.  
### 2.2 Τεχνητά Νευρωνικά Δίκτυα και Βαθιά Μάθηση  
  
Συχνά αναφέρονται απλώς ως νευρωνικά δίκτυα, τα τεχνητά νευρωνικά δίκτυα (ANNs) είναι υπολογιστικά μοντέλα που εμπνέονται από τη δομή και τον σκοπό των βιολογικών νευρικών συστημάτων. Ένα ANN αποτελείται θεμελιωδώς από **νευρώνες** που επεξεργάζονται σήματα εισόδου. Λαμβάνοντας αρκετές βαρυνόμενες εισόδους, ένα βασικό μοντέλο νευρώνα τις αθροίζει, προσθέτει έναν όρο **προκατάληψης** και περνά την έξοδο μέσω μιας **λειτουργίας ενεργοποίησης** (π.χ., Sigmoid, ReLU). Η μη γραμμικότητα που εισάγεται από αυτή τη συνάρτηση ενεργοποίησης βοηθά το δίκτυο να μάθει περίπλοκες αλληλεπιδράσεις.  
  
Αποτελούμενα από ένα επίπεδο εισόδου, ένα ή περισσότερα κρυφά επίπεδα και ένα επίπεδο εξόδου, τα **Πολυεπίπεδα Δίκτυα Τροφοδότησης** είναι η απλούστερη μορφή βαθιών νευρωνικών δικτύων. Χωρίς βρόχους, η πληροφορία κινείται σε μία κατεύθυνση—από την είσοδο προς την έξοδο. Συνήθως εφαρμόζοντας τον αλγόριθμο **backpropagation**, ο οποίος υπολογίζει το βαθμωτό της συνάρτησης απώλειας σε σχέση με τις παραμέτρους του δικτύου και τις ενημερώνει για να ελαχιστοποιήσει το σφάλμα, τα MLPs μαθαίνουν αλλάζοντας τα βάρη και τις προκαταλήψεις μέσω μιας επαναληπτικής διαδικασίας.  
  
Αν και τα MLPs είναι καλά για πολλές εργασίες, υποφέρουν με διαδοχικά δεδομένα όπου η ακολουθία και οι χρονικές εξαρτήσεις των εισροών είναι ζωτικής σημασίας. Τα επαναληπτικά νευρωνικά δίκτυα (RNNs) προορίζονται να λύσουν αυτό το πρόβλημα περιλαμβάνοντας βρόχους ή επαναληπτικές συνδέσεις που επιτρέπουν στα δεδομένα να παραμένουν σε διάφορα χρονικά βήματα, παρέχοντας έτσι στο δίκτυο μια μορφή **μνήμης**. Τα τυπικά RNNs δυσκολεύονται, επομένως, να μάθουν μακροχρόνιες εξαρτήσεις λόγω προκλήσεων όπως το πρόβλημα της εξαφανιζόμενης κλίσης.  
  
Η ανάπτυξη των δικτύων **Long Short-Term Memory (LSTM)** βοήθησε στην υπέρβαση αυτών των περιορισμών. Χρησιμοποιώντας εξελιγμένες **πύλες** (πύλες εισόδου, λήθης και εξόδου) που ελέγχουν τη ροή των πληροφοριών μέσα και έξω από το κύτταρο μνήμης του δικτύου, τα LSTM διαθέτουν μια πιο περίπλοκη αρχιτεκτονική. Ιδιαίτερα κατάλληλοι για την ανάλυση **χρηματοοικονομικών χρονοσειρών δεδομένων**, όπου οι προηγούμενες κινήσεις και τα μοτίβα τιμών μπορούν να επηρεάσουν τις μελλοντικές τάσεις, αυτός ο μηχανισμός πύλης επιτρέπει στα LSTMs να θυμούνται ή να ξεχνούν επιλεκτικά πληροφορίες σε εκτεταμένες ακολουθίες, επιτρέποντάς τους έτσι να συλλαμβάνουν μακροχρόνιες εξαρτήσεις. Μια σχετική αρχιτεκτονική με λιγότερες πύλες παρέχει μια συμπυκνωμένη μορφή των LSTM. **Μονάδες Επαναλαμβανόμενης Πύλης**.  
  
Τα συνελικτικά νευρωνικά δίκτυα (CNNs)** είναι μια άλλη κρίσιμη κατηγορία νευρωνικών δικτύων. Τα CNNs χρησιμοποιούν συνελικτικά στρώματα που εφαρμόζουν φίλτρα για να εντοπίσουν τοπικά μοτίβα, αν και η κύρια φήμη τους οφείλεται στην επιτυχία τους σε εργασίες επεξεργασίας εικόνας (π.χ., ταξινόμηση εικόνας, ανίχνευση αντικειμένων). Επιπλέον, είναι εφαρμόσιμα σε διαδοχικά δεδομένα ή χρησιμοποιούνται για εξαγωγή χαρακτηριστικών σε άλλα πεδία.  
  
**βαθιά μάθηση (DL)** είναι η εφαρμογή πολλαπλών κρυφών επιπέδων ΤΝΝ, οπότε "βαθιά". Η δύναμη της βαθιάς μάθησης έγκειται στην ικανότητά της να μαθαίνει αυτόματα ιεραρχικές αναπαραστάσεις ή χαρακτηριστικά απευθείας από ακατέργαστα δεδομένα, αφαιρώντας έτσι την ανάγκη για πολλές φορές χειροκίνητη μηχανική χαρακτηριστικών. Τα βαθιά δίκτυα μπορούν να μάθουν σταδιακά πιο αφηρημένα και εξελιγμένα χαρακτηριστικά σε διαδοχικά επίπεδα, στοιβάζοντας πολλά επίπεδα.

### 2.3 Βαθιά Ενισχυτική Μάθηση (DRL) για Trading  
  
Συνδυάζοντας τις ιδέες της Ενισχυτικής Μάθησης με την ικανότητα των Βαθιών Νευρωνικών Δικτύων (DNNs), η **Βαθιά Ενισχυτική Μάθηση (DRL)** προσφέρει μια ισχυρή μέθοδο. Η ΔΡΛ στο πλαίσιο του χρηματοοικονομικού εμπορίου διδάσκει έναν πράκτορα να αλληλεπιδρά με ένα προσομοιωμένο περιβάλλον αγοράς, ώστε να μάθει μια βέλτιστη εμπορική **πολιτική** μεγιστοποιώντας τις σωρευτικές χρηματοοικονομικές ανταμοιβές.  
  
Χρησιμοποιούμενοι ως **προσεγγιστές συναρτήσεων**, τα DNNs, όπως τα Πολυεπίπεδα Αντιληπτήρια ή, ιδιαίτερα σημαντικά για διαδοχικά χρηματοοικονομικά δεδομένα, τα **LSTMs** Βοηθούν τον πράκτορα να διαχειριστεί τους υψηλής διάστασης και συνεχείς χώρους καταστάσεων που είναι χαρακτηριστικοί των χρηματοοικονομικών αγορών, προσεγγίζοντας σημαντικά στοιχεία του προβλήματος ενίσχυσης μάθησης (RL), όπως η συνάρτηση πολιτικής (χαρτογράφηση καταστάσεων σε ενέργειες) ή η συνάρτηση αξίας (εκτίμηση της αναμενόμενης μελλοντικής ανταμοιβής των καταστάσεων ή ζευγών κατάστασης-ενέργειας).  
  
Δημοφιλής κατηγορία αλγορίθμων DRL γνωστή ως **μέθοδοι ηθοποιού-κριτικού** χρησιμοποιεί δύο ξεχωριστά νευρωνικά δίκτυα: ένα δίκτυο ηθοποιού που μαθαίνει την πολιτική και επιλέγει ποια ενέργεια να αναλάβει, και ένα δίκτυο κριτικού που μαθαίνει μια συνάρτηση αξίας για να αξιολογεί τις επιλεγμένες ενέργειες από τον ηθοποιό. Η αξιολόγηση του κριτικού προσφέρει στον ηθοποιό ένα σήμα μάθησης για να καθοδηγήσει την ανάπτυξη της πολιτικής του.  
  
Λόγω της κορυφαίας απόδοσής του και των ευνοϊκών χαρακτηριστικών σταθερότητας, αυτή η διατριβή χρησιμοποιεί τον αλγόριθμο **Proximal Policy Optimisation**. Ο PPO περιορίζει την αλλαγή στην πολιτική σε κάθε βήμα, επιτρέποντας έτσι σταθερές ενημερώσεις του πολιτικού κλίματος. Ο κεντρικός του μηχανισμός είναι η βελτιστοποίηση της **κομμένης υποκατάστατης αντικειμενικής συνάρτησης**.
  
Η ενημέρωση πολιτικής στο PPO βασίζεται στον **αναλογικό δείκτη πολιτικής**, ο οποίος ορίζεται ως η πιθανότητα λήψης της ενέργειας $a_t$ στην κατάσταση $s_t$ υπό την νέα πολιτική $\pi_\theta$ δια του πιθανοτήτων υπό την παλιά πολιτική $\pi_{\theta_{old}}$:  
$$ r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \quad \text{(Eq 3.1)} $$ Για να αποτραπούν οι υπερβολικά μεγάλες ενημερώσεις πολιτικής, το PPO περιορίζει αυτήν την αναλογία να κυμαίνεται εντός ενός μικρού διαστήματος γύρω από το 1:  
$$ r^{clip}_t(\theta) = \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \quad \text{(Eq 3.2)} $$
 $$ r^{clip}_t(\theta) = \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \quad \text{(Eq 3.2)} $$ 
όπου $\epsilon$ είναι μια μικρή υπερπαράμετρος. Η κύρια αντικειμενική συνάρτηση που βελτιστοποιείται από το PPO είναι ο κλιππάρειος υποκατάστατος στόχος:  
$$ L^{CLIP}(\theta) = \mathbb{E}_t[\min(r_t(\theta)\hat{A}_t, r^{clip}_t(\theta)\hat{A}_t)] \quad \text{(Eq 3.3)} $$
Αυτός ο στόχος παίρνει το ελάχιστο από δύο όρους: τον τυπικό στόχο του gradient πολιτικής βαρύμενο με τον λόγο $r_t(\theta)$ και την κομμένη έκδοση αυτού του στόχου. Αυτό διασφαλίζει ότι οι ενημερώσεις πολιτικής είναι περιορισμένες, ειδικά για ενέργειες με μεγάλες πλεονεκτικές, ενισχύοντας έτσι τη σταθερότητα. $\hat{A}_t$ αντιπροσωπεύει την εκτίμηση πλεονεκτήματος, ποσοτικοποιώντας πόσο καλύτερη είναι μια ενέργεια σε σύγκριση με τη μέση ενέργεια σε μια δεδομένη κατάσταση.  
  
Οι εκτιμήσεις πλεονεκτήματος στο PPO υπολογίζονται συνήθως χρησιμοποιώντας την **Γενικευμένη Εκτίμηση Πλεονεκτήματος (GAE)**. Η GAE λειαίνει τις εκτιμήσεις πλεονεκτήματος κατά μήκος μιας τροχιάς χρησιμοποιώντας έναν σταθμισμένο μέσο όρο των υπολοίπων Χρονικής Διαφοράς (TD). Το TD υπολειμματικό $\delta_t$ στο χρονικό βήμα $t$ υπολογίζεται με βάση την άμεση ανταμοιβή $R_{t+1}$ και τις εκτιμήσεις αξίας των τρεχουσών και επόμενων καταστάσεων:  
$$ \delta_t = R_{t+1} + \gamma V(s_{t+1}) - V(s_t) \quad \text{(Eq 3.4)} $$  
όπου $\gamma$ είναι ο παράγοντας έκπτωσης. Η εκτίμηση πλεονεκτήματος $\hat{A}_t$ είναι τότε το άθροισμα των εκπτώσεων υπολοίπων TD:  
$$ \hat{A}_t = \sum_{l=0}^{\infty} (\gamma\lambda)^l \delta_{t+l} \quad \text{(Eq 3.5)} $$ όπου $\lambda$ είναι μια άλλη υπερπαράμετρος που ελέγχει την ισορροπία bias-variance της εκτίμησης.  
όπου το $\lambda$ είναι μια άλλη υπερπαράμετρος που ελέγχει την ισορροπία bias-variance της εκτίμησης. Η εκπαίδευση συχνά χρησιμοποιεί μια **μνήμη επανάληψης εμπειρίας** για να αποθηκεύει τροχιές που έχουν δειγματοληφθεί από το περιβάλλον, επιτρέποντας εκτός πολιτικής ή ομαδικές ενημερώσεις.  
  
Το PPO είναι ιδιαίτερα **κατάλληλο για χρηματοοικονομικό εμπόριο** λόγω της φυσικής του σταθερότητας, κυρίως σχετιζόμενης με τον κομμένο στόχο και την αποδοτική εκτίμηση πλεονεκτήματος. Είναι πιο ανθεκτικό από τις απλούστερες τεχνικές βαθμωτής πολιτικής στις θορυβώδεις σημεία ανταμοιβής και τις μεταβαλλόμενες καταστάσεις που παρατηρούνται στις αγορές.  
  
Παρά τα πλεονεκτήματα των αλγορίθμων όπως ο PPO, η DRL στη χρηματοδότηση εξακολουθεί να έχει σημαντικές **προκλήσεις** κυρίως σχετικές με τη μη στασιμότητα που προκύπτει από τον υψηλό **θόρυβο** της αγοράς. Αυτές οι δυσκολίες απαιτούν την εξέταση επιπλέον στρατηγικών σταθεροποίησης για την επίτευξη αξιόπιστης και τακτικά κερδοφόρας εμπορικής απόδοσης.


[VISUALIZATION PLACEHOLDER: Type=Diagram, Title='Actor-Critic Architecture Overview', Data='Shows input state leading to both Actor and Critic networks. Actor outputs policy (action probabilities). Critic outputs value estimate. Critic's value used to calculate advantage, which feeds back to update Actor.']

### 2.4.1 Εισαγωγή στην Απόσταξη Γνώσης  
  
Μια μέθοδος μηχανικής μάθησης βασισμένη στην ιδέα της μεταφοράς αποκτηθείσας γνώσης από ένα μοντέλο "δάσκαλο" σε ένα μοντέλο "μαθητή" είναι η απόσταξη γνώσης (KD). Συνήθως, ο μαθητής είναι ένα μικρότερο, απλούστερο μοντέλο που προορίζεται για πιο αποτελεσματική ανάπτυξη ή ταχύτερη εκπαίδευση· ο δάσκαλος είναι συνήθως ένα μεγαλύτερο, πιο εξελιγμένο μοντέλο που έχει επιτύχει υψηλή απόδοση.  
  
Οι κύριοι στόχοι του KD είναι η **συμπίεση μοντέλου**, ώστε να επιτρέπεται η ανάπτυξη μικρότερων, ταχύτερων μοντέλων μαθητών με απόδοση κοντά σε αυτή του μεγαλύτερου δασκάλου· η **βελτίωση της απόδοσης του μαθητή**, ώστε να επιτρέπεται στον μαθητή να επιτύχει καλύτερη ακρίβεια ή ανθεκτικότητα από ό,τι θα μπορούσε να έχει αν εκπαιδευόταν μόνο με τα αρχικά δεδομένα αλήθειας· και η λειτουργία ως τεχνική **κανονικοποίησης** κατά την εκπαίδευση του μαθητή.  
  
Η θεμελιώδης ιδέα είναι το μοντέλο του μαθητή να μαθαίνει να μιμείται τη συμπεριφορά του δασκάλου. Ο μαθητής εκπαιδεύεται χρησιμοποιώντας "μαλακούς στόχους" που δημιουργούνται από την έξοδο του δασκάλου (π.χ., κατανομές πιθανοτήτων ανά κατηγορία) ή με την αντιστοίχιση των εσωτερικών αναπαραστάσεων του δασκάλου (π.χ., ενεργοποιήσεις χαρακτηριστικών από ενδιάμεσα επίπεδα), αποφεύγοντας έτσι την εξάρτηση μόνο από τις σκληρές ετικέτες αλήθειας. Αυτή η προσέγγιση επιτρέπει στον μαθητή να αποκτήσει πιο σύνθετη και πλούσια γνώση για τα δεδομένα και το έργο από ό,τι θα μπορούσε να προέλθει μόνο από την αληθή ετικέτα. Ανάλογα με την συγκεκριμένη τεχνική KD που χρησιμοποιείται, μπορεί κανείς να αποστάξει διάφορες μορφές γνώσης, συμπεριλαμβανομένων των πιθανοτήτων εξόδου ή των ενδιάμεσων ενεργοποιήσεων χαρακτηριστικών. 
  
### 2.4.2 Logit Matching: Κλασική Απόσταξη Γνώσης  
  
Συχνά χρησιμοποιούμενη σε προβλήματα ταξινόμησης, η πιο βασική και κοινή μορφή απόσταξης γνώσης είναι η μεταφορά γνώσης βασισμένη στην αντιστοίχιση της πιθανότητας εξόδου του μοντέλου του δασκάλου και του μαθητή (ή logits). Αυτή η προσέγγιση χρησιμοποιεί "μαλακούς στόχους," οι οποίοι είναι η συνάρτηση softmax με παράμετρο κλιμάκωσης θερμοκρασίας $T > 1$ που παράγεται από την τελευταία στρώση εξόδου του δασκάλου.
  
Για μια δεδομένη κατάσταση $s$ και δράση $a$, η μαλακή κατανομή πιθανότητας $q(a|s)$ που παράγεται από τα logits του δασκάλου $y^{(t)}(a|s)$ υπολογίζεται ως εξής:
$$ q(a|s) = \frac{\exp(y^{(t)}(a|s)/T)}{\sum_{a'} \exp(y^{(t)}(a'|s)/T)} \quad \text{(Eq 3.6)} $$Ομοίως, η μαλακή κατανομή πιθανοτήτων $p(a|s)$ για τα logits του μαθητή $y^{(s)}(a|s)$ υπολογίζεται χρησιμοποιώντας την ίδια θερμοκρασία. $T$:  
$$ p(a|s) = \frac{\exp(y^{(s)}(a|s)/T)}{\sum_{a'} \exp(y^{(s)}(a'|s)/T)} \quad \text{(Εξ. 3.7)} $$

Η χρήση σκληρών (one-hot) ετικετών οδηγεί σε απώλεια πληροφορίας· μια υψηλότερη θερμοκρασία $T$ μαλακώνει την κατανομή πιθανότητας, έτσι την εξομαλύνει και αποκαλύπτει τις σχετικές ομοιότητες μεταξύ διαφορετικών κατηγοριών ή δραστηριοτήτων όπως τις βλέπει ο δάσκαλος. Συνήθως, η ελαχιστοποίηση της απόκλισης—συνήθως της απόκλισης Kullback-Leibler (KL) ή της διασταυρούμενης εντροπίας—μεταξύ της μαλακής πιθανότητας του δασκάλου και της μαλακής πιθανότητας του μαθητή αποτελεί τη βάση αυτής της κλασικής μεθόδου KD: 
$$ L_{log} = \mathbb{E}_{s} \left[ D_{KL}(q(\cdot|s) || p(\cdot|s)) \right] \quad \text{(Eq 3.8)} $$
Η μείωση αυτής της απώλειας προάγει την κατανομή εξόδου του μαθητή να ταιριάζει με αυτή του δασκάλου, μεταφέροντας τα μαθαμένα όρια απόφασης και τα επίπεδα εμπιστοσύνης του δασκάλου.  
Η μείωση αυτής της απώλειας προάγει την κατανομή εξόδου του μαθητή ώστε να ταιριάζει με αυτήν του δασκάλου, μεταφέροντας έτσι τα μαθημένα όρια αποφάσεων και τα επίπεδα εμπιστοσύνης του δασκάλου.
### 2.4.3 Μεταφορά Πιθανότητας Γνώσης (PKT)  
  
Μια προηγμένη τεχνική απόσταξης γνώσης που ονομάζεται **πιθανωτική μεταφορά γνώσης (PKT)** πηγαίνει πέρα από την απλή αντιστοίχιση των εξόδων λογιστικών. Αντίθετα, δίνει έμφαση στη διάδοση της γνώσης που περιέχεται στα **ενδιάμεσα στρώματα** του συστήματος δασκάλου. Στοχεύοντας στη μεταφορά της υποκείμενης γεωμετρίας και των σχέσεων που γίνονται αντιληπτές από τις κρυφές αναπαραστάσεις του δασκάλου στο μοντέλο του μαθητή, το PKT επιδιώκει κυρίως να ταιριάξει την κατανομή πιθανότητας στον **χώρο χαρακτηριστικών** που έχει μάθει ο δάσκαλος.  
  
Βασισμένο σε ζεύγη σημείων δεδομένων $(x_i, x_j)$ στον χώρο χαρακτηριστικών ενός δεδομένου επιπέδου, το PKT ορίζει κατανομές συνθηκών πιθανότητας. Αυτή η κατανομή $p_{i|j}$ χαρακτηρίζεται για το δίκτυο δασκάλου από έναν συμμετρικό πυρήνα $K$.
$$ p_{i|j} = \frac{K(x_i, x_j; 2\sigma^2)}{\sum_{k \neq j} K(x_i, x_k; 2\sigma^2)} \quad \text{(Eq 3.9)} $$
Παρόμοια, για το αντίστοιχο ενδιάμεσο στρώμα του δικτύου του μαθητή, η κατανομή $q_{i|j}$ ορίζεται:  
Ομοίως, για το αντίστοιχο ενδιάμεσο επίπεδο του δικτύου του φοιτητή, η κατανομή $q_{i|j}$ ορίζεται ως εξής:  
$$ q_{i|j} = \frac{K(x_i, x_j; 2\sigma^2)} {\sum_{k \neq j} K(x_i, x_k; 2\sigma^2)} \quad \text{(Εξ. 3.10)} $$  
όπου $x_i$ και $x_j$ είναι διανύσματα χαρακτηριστικών για τα σημεία δεδομένων $i$ και $j$, και $\sigma^2$ είναι μια παράμετρος πλάτους πυρήνα. Αυτή η διατριβή χρησιμοποιεί τον **Πυρήνα Ομοιότητας Συνημιτόνου** για τα πλεονεκτήματά του, όπως το ότι δεν απαιτεί ρητή ρύθμιση πλάτους ζώνης:  
$$ K_{cosine}(a, b) = \frac{1}{2} \left( \frac{a \cdot b} {||a||_2 ||b||_2} + 1 \right) \quad \text{(Εξ. 4.3)} $$  
Η συνάρτηση απώλειας για το PKT ορίζεται ως η ελαχιστοποίηση της απόκλισης KL μεταξύ αυτών των συνθηκών κατανομών πιθανοτήτων για τον δάσκαλο και τον μαθητή σε μια παρτίδα δεδομένων:  
$$ L_{pkt} = \sum_{i} \sum_{j \neq i} p_{i|j} \log \left( \frac{p_{i|j}}{q_{i|j}} \right) \quad \text{(Eq 3.11)} $$
Η μείωση αυτής της απώλειας βοηθά τον χώρο χαρακτηριστικών του μαθητή να διατηρήσει τη τοπική δομή και τις συνδέσεις μεταξύ των σημείων δεδομένων όπως καταγράφηκαν από τον δάσκαλο, μεταδίδοντας έτσι μια διαφορετική, ίσως πλουσιότερη, μορφή γνώσης από τις τελικές πιθανότητες εξόδου.
  
### 2.4.4 Ensemble and Online Distillation Προσεγγίσεις 
  
Οι μέθοδοι απόσταξης γνώσης μπορούν να εφαρμοστούν για τη χρήση πολλών δασκάλων ή για την προσαρμογή σε δυναμικά μαθησιακά περιβάλλοντα εκτός της ανταλλαγής γνώσης μεταξύ ενός δασκάλου.  
  
Αντί να εξαρτάται μόνο από έναν δάσκαλο, το **Ensemble KD** χρησιμοποιεί μια ομάδα από διάφορα μοντέλα δασκάλων, συνήθως εκπαιδευμένα είτε ανεξάρτητα είτε με μικρές αποκλίσεις. Στη συνέχεια, οι πληροφορίες από αυτό το σύνολο συγκεντρώνονται για να δώσουν στον μαθητή ένα πιο ολοκληρωμένο και ίσως ισχυρότερο σήμα μάθησης. Ο μέσος όρος των ατομικών τους απωλειών εξάχνωσης είναι μια κοινή προσέγγιση για τη συγκέντρωση γνώσης από μια ομάδα $N$ δασκάλων. Η συνολική απώλεια απόσταξης για το σύνολο θα ήταν αν $L_{dist_i}$ είναι η απώλεια απόσταξης που υπολογίζεται με βάση τον δάσκαλο $i$.
 $$ L_{dist} = \frac{1}{N} \sum_{i=1}^{N} L_{dist_i} \quad \text{(Eq 3.13)} $$Η χρήση ενός συνόλου  καθηγητών μπορεί να βοηθήσει στην αποτύπωση ενός πιο γενικού μοντέλου μαθητή μέσω της αυξημένης **ποικιλίας** γνώσεων και της ενισχυμένης **ανθεκτικότητας** σε ατομικά λάθη ή προκαταλήψεις των δασκάλων.  
  
Σε αντίθεση με το συμβατικό **offline KD** παράδειγμα, όπου το μοντέλο δασκάλου είναι πλήρως προεκπαιδευμένο και οι παράμετροί του είναι καθορισμένες κατά τη διάρκεια της εκπαίδευσης του μαθητή, το **Online KD** σηματοδοτεί μια απομάκρυνση και από τα δύο. Η διαδικτυακή μεταφορά γνώσης (Online KD), από την άλλη πλευρά, περιλαμβάνει το μοντέλο δασκάλου και το μοντέλο μαθητή να εκπαιδεύονται **ταυτόχρονα** ή την συνεχιζόμενη προσαρμογή του μοντέλου δασκάλου κατά τη διάρκεια της εκπαιδευτικής πορείας του μαθητή. Αυτό επιτρέπει μια πιο δυναμική μεταφορά γνώσης, όπου η γνώση του δασκάλου αλλάζει με τη μάθηση του μαθητή. Η διαδικτυακή μεταφορά γνώσης είναι ιδιαίτερα **κατάλληλη για δυναμικά περιβάλλοντα**, όπως οι χρηματοπιστωτικές αγορές, όπου τα μοτίβα και οι βέλτιστες στρατηγικές μπορούν να εξελιχθούν με την πάροδο του χρόνου, καθώς επιτρέπει στη διαδικασία μεταφοράς γνώσης να προσαρμόζεται σε αυτές τις μεταβαλλόμενες συνθήκες.

### 2.4.5 Combined Loss Function for DRL with KD
  
Συνήθως, συνδυάζοντας την τυπική απώλεια RL με την απώλεια απόσταξης, η Απόσταξη Γνώσης ενσωματώνεται σε ένα πλαίσιο Βαθιάς Ενισχυτικής Μάθησης. Οι παράμετροι του πράκτορα αλλάζουν για να μεγιστοποιήσουν τη συνολική ανταμοιβή από την αλληλεπίδραση με το περιβάλλον και να αναπαράγουν τη γνώση από τον/τους δάσκαλο/ους, ελαχιστοποιώντας έτσι μια συνδυασμένη συνάρτηση απώλειας.  
  
Ο πράκτορας DRL που περιλαμβάνει KD μπορεί να έχει μια **συνδυασμένη απώλεια** δομή που εμφανίζεται ως εξής:
$$ L = L_{RL} + \beta \cdot L_{dist} \quad \text{(Eq 3.12)} $$
Εδώ, το $L_{dist}$ δείχνει την απώλεια απόσταξης, η οποία θα μπορούσε να είναι $L_{log}$ (Εξ. 3.8), $L_{pkt}< A$ υπερπαράμετρος βάρους, $\beta$ ρυθμίζει τη σχετική σημασία της απώλειας απόσταξης σε σχέση με την απώλεια RL.  
  
Ενσωματώνοντας διάφορους τύπους απόσταξης—ταίριασμα λογιστικών καθώς και PKT—η απώλεια απόσταξης $L_{dist}$ μπορεί να είναι ένα σταθμισμένο άθροισμα.
$$ L_{dist} = \beta_1 \cdot L_{log} + \beta_2 \cdot L_{pkt} $$  
οδηγώντας σε μια συνδυασμένη δομή απώλειας όπως:  
$$ L = L_{RL} + \beta_1 \cdot L_{log} + \beta_2 \cdot L_{pkt} \quad \text{(Εξ. 3.14)} $$ όπου $\beta_1$ και $\beta_2$ είναι υπερπαράμετροι που ζυγίζουν τη συμβολή κάθε όρου απόσταξης.  
όπου $\beta_1$ και $\beta_2$ είναι υπερπαράμετροι που ζυγίζουν τη συμβολή κάθε όρου απόσταξης. Αυτός ο συνδυασμένος στόχος επιτρέπει στον πράκτορα DRL να μάθει τόσο από την άμεση περιβαλλοντική ανατροφοδότηση όσο και από την καθοδηγούμενη καθοδήγηση που παρέχεται από τον/τους δάσκαλο/ους μέσω διαφορετικών μορφών μεταφοράς γνώσης.  
### 2.5.1 Χρηματοπιστωτικές Αγορές και Κρυπτονόμισμα  
  
Όπου οι τιμές καθορίζονται κυρίως από την αλληλεπίδραση της προσφοράς και της ζήτησης, οι χρηματοπιστωτικές αγορές παρέχουν χώρους για την εμπορία διαφόρων περιουσιακών στοιχείων, συμπεριλαμβανομένων των μετοχών, των ομολόγων, των εμπορευμάτων και των νομισμάτων. Αυτές οι αγορές είναι περίπλοκα συστήματα που διαμορφώνονται από μια ποικιλία οικονομικών, πολιτικών και ψυχολογικών παραγόντων, ως πλατφόρμες όπου διαπραγματεύονται διάφορα περιουσιακά στοιχεία, συμπεριλαμβανομένων μετοχών, ομολόγων, νομισμάτων και εμπορευμάτων. Η αλληλεπίδραση της προσφοράς και της ζήτησης, που διαμορφώνεται από πολλά πολιτικά, κοινωνικά και οικονομικά στοιχεία, καθορίζει κυρίως τις τιμές σε αυτές τις αγορές.  
  
Μια μάλλον πρόσφατη κατηγορία ψηφιακών ή εικονικών νομισμάτων είναι τα **κρυπτονομίσματα**. Συνήθως καταγράφονται σε μια τεχνολογία κατανεμημένου καθολικού που ονομάζεται blockchain, και ορίζονται από τον διασκορπισμένο τους χαρακτήρα—δηλαδή, από τη λειτουργία τους χωρίς κεντρική τράπεζα ή διαχειριστή—και από την κρυπτογραφία.  
  
Ορισμένα σημαντικά χαρακτηριστικά των αγορών κρυπτονομισμάτων τις διαφοροποιούν από τις συμβατικές χρηματοπιστωτικές αγορές και βοηθούν στην ανάπτυξη αλγοριθμικών εμπορικών πρακτόρων:  
Οι τιμές των κρυπτονομισμάτων είναι γνωστές για τις μεγάλες και γρήγορες διακυμάνσεις τους, οι οποίες προσφέρουν στους εμπόρους τόσο ευκαιρίες όσο και κινδύνους.  
Σε αντίθεση με πολλές συμβατικές αγορές, οι ανταλλαγές κρυπτονομισμάτων λειτουργούν 24 ώρες το 24ωρο, 7 ημέρες την εβδομάδα, επομένως η συμπερίληψη πολλών πηγών πληροφόρησης θα μπορούσε να είναι πολύ ωφέλιμη για την πρόβλεψη και τη λήψη αποφάσεων, καθώς οι τιμές τους είναι συχνά πολύ αντιδραστικές σε ειδήσεις, συναισθήματα στα μέσα κοινωνικής δικτύωσης, νομοθετικές εξελίξεις και άλλα εξωτερικά γεγονότα. 
### 2.5.2 Αναπαράσταση Δεδομένων Candlestick  
  
Ένα κοινό και ευρέως χρησιμοποιούμενο εργαλείο για την απεικόνιση των κινήσεων τιμών ενός χρηματοοικονομικού περιουσιακού στοιχείου σε μια δεδομένη περίοδο είναι το διάγραμμα κηροπήγιο. Τέσσερα κύρια σημεία τιμών μέσα σε αυτή την περίοδο—οι τιμές **Άνοιγμα**, **Υψηλό**, **Χαμηλό** και **Κλείσιμο** (OHLC)—παρουσιάζονται συνοπτικά από κάθε κερί.  
  
Δύο κύρια στοιχεία αποτελούν ένα μόνο κερί: το **'σώμα'** και τις **'φυτίλια'** ή **'σκιές'**. Το σώμα και οι σκιές. Το εύρος μεταξύ των τιμών Άνοιγμα και Κλείσιμο για την περίοδο εμφανίζεται στο σώμα του κηροπήγιου. Γραμμές που υποδεικνύουν τις υψηλές και χαμηλές τιμές που επιτεύχθηκαν κατά την ίδια περίοδο τρέχουν από την κορυφή και το κάτω μέρος του σώματος, οι σκιές ή τα φυτίλια.  
  
Συνήθως κωδικοποιημένο με χρώματα για να δείξει γρήγορα αν η τιμή αυξήθηκε ή μειώθηκε κατά την περίοδο, το σώμα ενός καπνοσυλλέκτη είναι Γενικά μιλώντας, ένα **πράσινο ή λευκό σώμα** υποδηλώνει αύξηση της τιμής, καθώς η τιμή κλεισίματος ήταν υψηλότερη από την τιμή ανοίγματος. Από την άλλη πλευρά, ένα **κόκκινο ή μαύρο σώμα** δείχνει ότι υπήρξε πτώση της τιμής, καθώς η τιμή κλεισίματος ήταν χαμηλότερη από την τιμή ανοίγματος.  
  
**Η τεχνική ανάλυση** αξιοποιεί σε μεγάλο βαθμό τις υποψήφιες τάσεις, τις αναστροφές και άλλα σήματα της αγοράς που προέρχονται από ακολουθίες αυτών των οπτικών αναπαραστάσεων. Για τα συστήματα αλγοριθμικής διαπραγμάτευσης, αυτή η οπτική επισκόπηση της κίνησης των τιμών καθιστά τα κηροπήγια ένα ανεκτίμητο εργαλείο. Ο Πίνακας 1.1 παρέχει ένα παράδειγμα που δείχνει τη δομή των δεδομένων των κηροπηγίων για ένα συγκεκριμένο περιουσιακό στοιχείο σε διαδοχικά χρονικά διαστήματα.
  
### 2.5.3 Θέσεις Συναλλαγών  
  
Στο χρηματοοικονομικό εμπόριο, μια **θέση** είναι η έκθεση ενός trader σε ένα συγκεκριμένο περιουσιακό στοιχείο που αντικατοπτρίζει την ιδιοκτησία ή μια συμβατική συμφωνία σχετικά με την κίνηση της τιμής του. Συνήθως ξεκινώντας με μια εντολή αγοράς ή πώλησης, η δημιουργία μιας θέσης ονομάζεται **άνοιγμα** θέσης. Το κλείσιμο μιας θέσης είναι η διαδικασία της αποχώρησης από αυτήν, μειώνοντας έτσι την έκθεση στο περιουσιακό στοιχείο.  
  
Οι κύριες κατηγορίες ρόλων είναι: 
* **Μακρά Θέση** Αυτό είναι προφανές όταν ένας έμπορος αγοράζει ένα περιουσιακό στοιχείο ελπίζοντας ότι η τιμή του θα αυξηθεί. Οι μακροχρόνιες θέσεις αποδίδουν αν η τιμή του περιουσιακού στοιχείου αυξηθεί.  
* **Η βραχυπρόθεσμη θέση είναι:** Αυτό καθορίζεται όταν ένας έμπορος αναμένει ότι η τιμή ενός περιουσιακού στοιχείου που δεν κατέχει—συνήθως δανεισμένο—θα πέσει. Αν η τιμή του περιουσιακού στοιχείου μειωθεί, μια βραχυπρόθεσμη θέση ωφελεί καθώς επιτρέπει στον έμπορο να το αγοράσει αργότερα σε μειωμένη τιμή, επιστρέφοντας έτσι το δανεισμένο περιουσιακό στοιχείο. Οι βραχυπρόθεσες θέσεις είναι σχετικές ανάλογα με το συγκεκριμένο περιβάλλον συναλλαγών και τον καθορισμένο χώρο δράσης του πράκτορα DRL.  
* **Ουδέτερη Θέση:** Αυτό περιγράφει ούτε μακρά ούτε σύντομη απουσία έκθεσης στην αγορά για ένα συγκεκριμένο περιουσιακό στοιχείο.  
  
Είτε μια συναλλαγή καταλήξει σε **κέρδος είτε σε ζημία**, το κλείσιμο μιας θέσης σηματοδοτεί το οικονομικό αποτέλεσμα της συναλλαγής. Η διαφορά μεταξύ των τιμών ανοίγματος και κλεισίματος, διορθωμένη για το μέγεθος της θέσης, καθορίζει αυτό το αποτέλεσμα. Το πραγματικό εμπόριο περιλαμβάνει επίσης **κόστος συναλλαγών**, κυρίως προμήθειες που πληρώνονται στον μεσίτη ή το χρηματιστήριο, οι οποίες επηρεάζουν το καθαρό κέρδος ή ζημία. 
### 2.5.4 Υπολογισμός Κέρδους και Ζημίας  
  
Ο προσδιορισμός της κερδοφορίας και της αποτελεσματικότητας μιας στρατηγικής συναλλαγών εξαρτάται από την απόδοσή της, επομένως η αξιολόγησή της είναι απαραίτητη. Το Κέρδος και η Ζημία (PnL)** είναι ένα κοινό και γενικά αποδεκτό μέτρο των αποδόσεων που παράγονται από μια στρατηγική συναλλαγών σε μια δεδομένη περίοδο.  
  
Υπολογίζεται ως το σύνολο των αποδόσεων που επιτυγχάνονται σε κάθε μεμονωμένο χρονικό βήμα, το PnL για μια περίοδο αξιολόγησης $N$ χρονικών βημάτων:
$$ \text{PnL} = \sum_{t=1}^{N} \text{return}_t \quad \text{(Εξ 1.5)} $$ Εδώ, $\text{return}_t$ είναι το κέρδος ή η ζημία της στρατηγικής συναλλαγών στο χρονικό βήμα $t$. Συνήθως, λαμβάνοντας υπόψη τα κόστη συναλλαγών, αυτή η απόδοση υπολογίζεται ανάλογα με την αλλαγή στην αξία του χαρτοφυλακίου του πράκτορα ή το αποτέλεσμα των συναλλαγών κατά τη διάρκεια αυτού του βήματος.  
  
Εφαρμοσμένο στο **backtesting**—μια τεχνική που προσομοιώνει την απόδοση μιας στρατηγικής συναλλαγών σε δεδομένα της αγοράς του παρελθόντος—το μέτρο PnL είναι ιδιαίτερα χρήσιμο. Ο υπολογισμός του PnL σε μια ιστορική περίοδο βοηθά τους traders και τους ερευνητές να αξιολογήσουν την επιτυχία της στρατηγικής υπό τις προηγούμενες συνθήκες της αγοράς, να εντοπίσουν τα πλεονεκτήματα και τα μειονεκτήματά της, και να τη συγκρίνουν με άλλες προσεγγίσεις ή benchmarks.

### 2.6 Σχετική Βιβλιογραφία  
  
Τονίζοντας τις τρέχουσες εφαρμογές της Βαθιάς Ενισχυτικής Μάθησης (DRL) στο χρηματοοικονομικό εμπόριο και την σχετική εργασία στην Απόσταξη Γνώσης (KD), αυτό το μέρος συνοψίζει προηγούμενη έρευνα που είναι σχετική με αυτή τη διατριβή. Η πρόθεση είναι να καθοριστεί σαφώς το συγκεκριμένο ερευνητικό κενό που καλύπτει η παρούσα εργασία και να τοποθετηθεί η παρούσα εργασία στο πλαίσιο.  
  
**DRL στις Χρηματοοικονομικές Συναλλαγές:** Η έρευνα για την εφαρμογή της DRL στο αυτοματοποιημένο χρηματοοικονομικό εμπόριο έχει είναι αρκετά ενεργή. Προσεγγίσεις βασισμένες στην αξία, συμπεριλαμβανομένου του Q-learning και των βαθιών παραλλαγών του (DQN), μεθόδων βαθμωτής πολιτικής, και ειδικότερα αλγορίθμων Actor-Critic όπως A2C/A3C και Proximal Policy Optimisation (PPO) έχουν μελετηθεί για την εκμάθηση πολιτικών συναλλαγών. Μέσω αλληλεπιδράσεων με προσομοιώσεις αγοράς και βελτιστοποίησης στόχων όπως το σωρευτικό κέρδος ή ο δείκτης Sharpe, αυτές οι μελέτες έχουν δείξει την ικανότητα των πρακτόρων DRL να μαθαίνουν περίπλοκες στρατηγικές. Ωστόσο, ένα συνεπές αποτέλεσμα σε μεγάλο μέρος της βιβλιογραφίας είναι η αξιοσημείωτη **αστάθεια εκπαίδευσης** των πρακτόρων DRL σε δυναμικά και θορυβώδη χρηματοοικονομικά περιβάλλοντα. Συχνά εμφανιζόμενη ως υψηλή **ευαισθησία στον θόρυβο**, αυτή η αστάθεια προκαλεί σημαντική **μεταβλητότητα στην απόδοση** σε πολλές εκπαιδευτικές δοκιμές ή υπό παρατηρούμενες συνθήκες της αγοράς. Προηγούμενες μέθοδοι έχουν προσπαθήσει να λύσουν αυτά τα προβλήματα μέσω της βελτιωμένης διαμόρφωσης ανταμοιβών, της συμπερίληψης βοηθητικών εργασιών ή των αλλαγών στις αρχιτεκτονικές αλγορίθμων. Παρά αυτές τις πρωτοβουλίες, ένα σημαντικό εμπόδιο και ανοιχτή ερευνητική πρόκληση παραμένει η επίτευξη ισχυρής και σταθερά σταθερής εκπαίδευσης και απόδοσης DRL στις χρηματοπιστωτικές αγορές.  
  
**Η απόσταξη γνώσης (KD) είναι:** Αρχικά παρουσιάστηκε για τη συμπίεση μοντέλων, η απόσταξη γνώσης έχει εξελιχθεί σε μια πιο γενική μέθοδο μεταφοράς γνώσης μεταξύ νευρωνικών δικτύων. Συνήθως χρησιμοποιώντας κλιμάκωση θερμοκρασίας και ελαχιστοποίηση της απόκλισης (π.χ., KL απόκλιση), οι τυπικές μέθοδοι KD συνήθως περιλαμβάνουν την εκπαίδευση ενός μικρότερου δικτύου μαθητή για να αναπαράγει τις μαλακές πιθανότητες εξόδου (logits) ενός μεγαλύτερου, προεκπαιδευμένου μοντέλου δασκάλου. Πέρα από τη συμπίεση, η KD έχει χρησιμοποιηθεί αποτελεσματικά για να παρέχει ένα πλουσιότερο σήμα μάθησης από τα σκληρά ετικέτες μόνο, βελτιώνοντας έτσι την απόδοση των μοντέλων μαθητών σε πολλές δραστηριότητες επιβλεπόμενης μάθησης, ειδικά στην ταξινόμηση. Πιο πρόσφατα, οι ιδέες της KD έχουν επεκταθεί στη Μηχανική Μάθηση Ενίσχυσης, που μερικές φορές ονομάζεται Απόσταξη Πολιτικής ή Απόσταξη Αξίας, όπου ένας μαθητής πράκτορας RL μαθαίνει αντιγράφοντας την πολιτική ή τη λειτουργία αξίας ενός δασκάλου πράκτορα RL. Αυτά τα προγράμματα έχουν δείξει υποσχέσεις στην ενίσχυση της αποδοτικότητας των δειγμάτων ή στην εφαρμογή πολύπλοκων συμπεριφορών που έχουν διδαχθεί από προχωρημένους δασκάλους.  
  
**Κενό Έρευνας:** Αν και η DRL έχει χρησιμοποιηθεί στο χρηματοοικονομικό εμπόριο και η KD έχει διερευνηθεί στη RL, η συγκεκριμένη εφαρμογή των **προηγμένων μεθόδων Απόσταξης Γνώσης** για την άμεση αντιμετώπιση των κρίσιμων ζητημάτων της **αστάθειας εκπαίδευσης** και τη βελτίωση της **απόδοσης εμπορίου** των πρακτόρων DRL σε ασταθείς **αγορές κρυπτονομισμάτων** χρησιμοποιώντας μόνο **μηχανικά χαρακτηριστικά τιμών** παραμένει μια υποενεργή περιοχή. Προηγούμενη εργασία έχει δείξει την αστάθεια του χρηματοοικονομικού DRL και έχει διερευνήσει κάποιες βασικές τεχνικές KD στο RL· αλλά, μια συστηματική διερεύνηση του κατά πόσο οι προηγμένες τεχνικές KD, συγκεκριμένα η **Πιθανοτική Μεταφορά Γνώσης (PKT)** από ενδιάμεσα επίπεδα δικτύου και οι προσεγγίσεις **διαδικτυακής συνδυαστικής απόσταξης**, μπορούν να λειτουργήσουν αποτελεσματικά ως ισχυροί μηχανισμοί σταθεροποίησης και να οδηγήσουν σε μετρήσιμα ανώτερα αποτελέσματα συναλλαγών σε αυτόν τον απαιτητικό τομέα, λείπει. Αυτή η διατριβή προτείνει, υλοποιεί και αξιολογεί εμπειρικά ένα πλαίσιο DRL που ενσωματώνει αυτές τις προηγμένες τεχνικές KD για να βελτιώσει τη σταθερότητα και την κερδοφορία των πρακτόρων συναλλαγών κρυπτονομισμάτων που βασίζονται μόνο σε τιμές εισόδων, γεφυρώνοντας έτσι αυτό το συγκεκριμένο ερευνητικό κενό.
## 3.1 Προσέγγιση Ενισχυτικής Μάθησης  
  
Σε αυτή την ενότητα, παρέχεται μια σύντομη επισκόπηση της μεθοδολογίας Ενισχυτικής Μάθησης. Στη συνέχεια, περιγράφεται λεπτομερώς η κλασική μέθοδος απόσταξης γνώσης και η πιθανοτική μεταφορά γνώσης, ακολουθούμενη από την προτεινόμενη αρχιτεκτονική απόσταξης γνώσης. Πιο συγκεκριμένα, περιγράφουμε τα πλεονεκτήματα της απόσταξης της γνώσης ενός συνόλου δασκάλων, καθώς και το σχέδιο διαδικτυακής απόσταξης.  
  
Αυτή η διατριβή χρησιμοποιεί τη μέθοδο **Proximal Policy Optimization (PPO)** για την εκπαίδευση του μοντέλου DRL, το οποίο έχει επιτύχει αποτελέσματα αιχμής σε πράκτορες που έχουν εκπαιδευτεί με αυτήν, όπως αποδεικνύεται σε έργα [4], [18].  
  
Παρουσιάζουμε τον αλγόριθμο Proximal Policy Optimization (PPO) που χρησιμοποιεί τον λόγο πιθανότητας δράσης μεταξύ της παραμετροποίησης πολιτικής σε διάφορα βήματα. Ο **λόγος πολιτικής** $r_t(\theta)$ υπολογίζεται ως εξής:  
$$ r_t(\theta) = \frac{\pi_\theta(a_t|s_t)} {\pi_{\theta_{old}}(a_t|s_t)} \quad \text{(Εξ. 3.1)} $$  
όπου $\pi_\theta(a_t|s_t)$ είναι η πιθανότητα ότι η νέα πολιτική $\pi_\theta$ θα επιλέξει την ενέργεια $a_t$ δεδομένης της κατάστασης του περιβάλλοντος $s_t$ στο χρονικό βήμα $t$, και $\pi_{\theta_{old}}(a_t|s_t)$ είναι η πιθανότητα υπό την παλιά πολιτική. Για να ελεγχθεί η εξερεύνηση της πολιτικής και να αποτραπούν οι μεγάλες ενημερώσεις, χρησιμοποιείται μια **κομμένη έκδοση του λόγου**, που ορίζεται ως εξής:  
$$ r^{clip}_t(\theta) = \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)  \quad \text{(Εξ. 3.2)} $$
όπου το $\epsilon$ είναι μια μικρή υπερπαραμετρος που ελέγχει το εύρος κλιπ της ενημέρωσης πολιτικής. Η τελική συνάρτηση στόχος που βελτιστοποιείται από το PPO είναι ο **κομμένος παρεμβατικός στόχος**:  
$$ L^{CLIP}(\theta) = \mathbb{E}_t[\min(r_t(\theta)\hat{A}_t, r^{clip}_t(\theta)\hat{A}_t)] \quad \text{(Εξ. 3.3)} $$ όπου $\hat{A}_t$ είναι η **Γενικευμένη Εκτίμηση Πλεονεκτήματος (GAE)** ενός βήματος εντός της τροχιάς, η οποία προτάθηκε στο [15].  
όπου $\hat{A}_t$ είναι η **Γενικευμένη Εκτίμηση Πλεονεκτήματος (GAE)** ενός βήματος εντός της τροχιάς, η οποία προτάθηκε στο [15]. Ο μηχανισμός της λήψης του ελάχιστου μεταξύ $r_t(\theta)\hat{A}_t$ και $r^ {clip}_t(\theta)\hat{A}_t$ παρέχει έναν τρόπο περιορισμού των ενημερώσεων πολιτικής. Συγκεκριμένα, περιορίζει την κίνητρο του πράκτορα να αυξήσει την πιθανότητα δράσεων που ήδη εκτιμώνται ως πολύ καλές (θετικό πλεονέκτημα $\hat{A}_t$), και αντιστρόφως, αποτρέπει σημαντικές ποινές για δράσεις που εκτιμώνται ως πολύ κακές (αρνητικό πλεονέκτημα $\hat{A}_t$), προάγοντας έτσι μια πιο συντηρητική και σταθερή ενημέρωση.  
  
Ο υπολογισμός της Γενικευμένης Εκτίμησης Πλεονεκτήματος (GAE) $\hat{A}_t$ βασίζεται στο **υπόλοιπο Χρονικής Διαφοράς (TD)** $\delta_t$ για κάθε χρονικό βήμα $t$, που ορίζεται ως:  
$$ \delta_t = R_{t+1} + \gamma V(s_{t+1}) - V(s_t) \quad \text{(Εξ. 3.4)} $$  
όπου $R_{t+1}$ είναι η ανταμοιβή που λαμβάνεται στο βήμα $t+1$, $V(s_t)$ και $V(s_{t+1})$ είναι οι εκτιμήσεις αξίας για τις καταστάσεις $s_t$ και $s_{t+1}$ αντίστοιχα υπό την τρέχουσα πολιτική, και $\gamma$ είναι ο παράγοντας απόσβεσης. Ο GAE $\hat{A}_t$ υπολογίζεται στη συνέχεια ως μια προεξοφλημένη άθροιση αυτών των υπολειμμάτων TD:  
$$ \hat{A}_t = \sum_{l=0}^{\infty} (\gamma\lambda)^l \delta_{t+l} \quad \text{(Εξ. 3.5)} $$ όπου $\lambda$ είναι μια παράμετρος που εξισορροπεί την εμπορική-παραλλακτικότητα στην εκτίμηση του πλεονεκτήματος.  
όπου το $\lambda$ είναι μια παράμετρος που ισορροπεί την εμπορική συμφωνία bias-variance στην εκτίμηση πλεονεκτήματος. Η εκπαίδευση του πράκτορα DRL περιλαμβάνει τη συλλογή τροχιών μέσω αλληλεπίδρασης με το περιβάλλον και την αποθήκευση των σχετικών δεδομένων (καταστάσεις, ενέργειες, ανταμοιβές, εκτιμήσεις αξίας, πιθανότητες ενεργειών) σε μια **μνήμη επανάληψης εμπειρίας**. Στη συνέχεια, δείγματα δεδομένων λαμβάνονται από αυτή τη μνήμη για να γίνουν ενημερώσεις στις παραμέτρους τόσο του ηθοποιού (πολιτική) όσο και του κριτικού (λειτουργία αξίας) στοιχείου του πράκτορα DRL.

## 3.2 Πιθανιστική Απόσταξη Γνώσης  
  
Αυτή η ενότητα εμβαθύνει στις συγκεκριμένες τεχνικές Απόσταξης Γνώσης (KD) που χρησιμοποιούνται στο προτεινόμενο πλαίσιο DRL.  
  
Όπως εισάγεται στο Κεφάλαιο 2, η KD διευκολύνει τη μεταφορά της αποκτηθείσας γνώσης από ένα ισχυρότερο μοντέλο δασκάλου ή σύνολο σε ένα μοντέλο μαθητή. Αυτή η διαδικασία μπορεί να προσφέρει σημαντικά οφέλη, συμπεριλαμβανομένης της πιο αποτελεσματικής και σταθερής εκπαίδευσης, της πιθανής συμπίεσης του μοντέλου [28] και της κανονικοποίησης. Η προτεινόμενη μέθοδος χρησιμοποιεί τόσο την κλασική αντιστοίχιση λογιστικού όσο και τη προχωρημένη Μεταφορά Πιθανοτήτων Γνώσης (PKT). Πρώτα περιγράφουμε την έννοια της κλασικής αντιστοίχισης λογιστικού KD όπως εφαρμόζεται σε μια ρύθμιση DRL. Σε αυτό το παράδειγμα, η γνώση μεταφέρεται από το επίπεδο εξόδου του δασκάλου με την αντιστοίχιση των κατανομών πιθανοτήτων πάνω στις ενέργειες. Ορίζουμε την κατανομή πιθανοτήτων δράσης που παράγεται από την πολιτική του μοντέλου δασκάλου ως $\pi^{(t)}(a|s)$, και την έξοδο της πολιτικής του μοντέλου μαθητή ως $\pi^{(s)}(a|s)$. Οι ακατέργαστες εξόδους των δικτύων πριν από την τελική ενεργοποίηση softmax αναφέρονται ως logits, $y^{(t)}(a|s)$ για τον δάσκαλο και $y^{(s)}(a|s)$ για τον μαθητή. Η μαλακή κατανομή πιθανότητας $q(a|s)$ που παράγεται από τα logits του δασκάλου χρησιμοποιώντας μια παράμετρο κλιμάκωσης θερμοκρασίας $T$ υπολογίζεται ως εξής: $$ q(a|s) = \frac {\exp(y^{(t)}(a|s)/T)} {\sum_{a'} \exp(y^{(t)}(a'|s)/T)} \quad \text{(Εξ. 3.6)} $$ Ομοίως, η μαλακή κατανομή πιθανότητας $p(a|s)$ για τα logits του φοιτητή $y^{(s)}(a|s)$ υπολογίζεται χρησιμοποιώντας την ίδια θερμοκρασία $T$: $$ p(a|s) = \frac{\exp(y^{(s)}(a|s)/T)} {\sum_{a'} \exp(y^{(s)}(a'|s)/T)} \quad \text{(Εξ. 3.7)} $$ Η κλασική απώλεια αντιστοίχισης λογιστικού, $L_{log}$, στοχεύει στη μείωση της απόκλισης (π.χ., KL απόκλιση) μεταξύ των μαλακών πιθανοτήτων του μαθητή και των μαλακών πιθανοτήτων του δασκάλου, μεταφέροντας έτσι γνώση σχετικά με την σχετική πιθανότητα διαφορετικών ενεργειών όπως αυτές γίνονται αντιληπτές από το επίπεδο εξόδου του δασκάλου: $$ L_{log} = \mathbb{E}_{s} \left[ D_{KL}(q(\cdot|s) || p(\cdot|s)) \right] $$ \quad \text{(Εξ. 3.8)} Αυτός ο μέθοδος μεταφέρει κυρίως γνώσεις από το επίπεδο εξόδου του δικτύου. Αυτή η μέθοδος μεταφέρει κυρίως γνώση από το επίπεδο εξόδου του δικτύου. Το Σχήμα 3.1 απεικονίζει μια τυπική αρχιτεκτονική Actor-Critic όπου θα μπορούσε να εφαρμοστεί η απόσταξη στην έξοδο του ηθοποιού. Η προτεινόμενη μέθοδος περιλαμβάνει μια συγκεκριμένη μέθοδο **Μεταφοράς Πιθανότητας Γνώσης (PKT)** [16] που ξεπερνά την αντιστοίχιση εξόδου μεταφέροντας γνώση από **ενδιάμεσα στρώματα** του δικτύου του δασκάλου. Η βασική ιδέα είναι να ελαχιστοποιηθεί η απόκλιση μεταξύ των κατανομών πιθανοτήτων στον **χώρο χαρακτηριστικών** που έχει μάθει ο δάσκαλος και ο μαθητής, ανακατασκευάζοντας αποτελεσματικά την **τοπική γεωμετρία** του χώρου χαρακτηριστικών του δασκάλου μέσα στον χώρο του μαθητή. Αυτό επιτυγχάνεται με τον καθορισμό κατανομών πιθανότητας υπό συνθήκη με βάση τις ζεύγους ομοιότητες μεταξύ των σημείων δεδομένων $(x_i, x_j)$ στον χώρο χαρακτηριστικών ενός επιλεγμένου ενδιάμεσου στρώματος. Για το δίκτυο δασκάλου, η κατανομή πιθανότητας υπό συνθήκη $p_{i|j}$ μεταξύ των σημείων δεδομένων $i$ και $j$ (με διανύσματα χαρακτηριστικών $x_i$ και $x_j$) ορίζεται χρησιμοποιώντας έναν συμμετρικό πυρήνα $K$ με πλάτος $\sigma^2$: $$ p_{i|j} = \frac{K(x_i, x_j; 2\sigma^2)} {\sum_{k \neq j} K(x_i, x_k; 2\sigma^2)} \quad \text{(Εξ. 3.9)} $$ Ομοίως, για το αντίστοιχο ενδιάμεσο επίπεδο του δικτύου του μαθητή, ορίζεται η κατανομή πιθανότητας υπό συνθήκη $q_{i|j}$: $$ q_{i|j} = \frac{K(x_i, x_j; 2\sigma^2)} {\sum_{k \neq j} K(x_i, x_k; 2\sigma^2)} \quad \text{(Εξ. 3.10)}$$ Η συνάρτηση απώλειας PKT, $L_{pkt}$$, ορίζεται ως η ελαχιστοποίηση της KL απόκλισης μεταξύ αυτών των κατανομών πιθανότητας υπό συνθήκη για τον δάσκαλο και τον μαθητή σε μια παρτίδα δεδομένων: $$ L_{pkt} = \sum_{i} \sum_{j \neq i} p_{i|j} \log \left( \frac{p_{i|j}}{q_{i|j}} \right) \quad \text{(Eq 3.11)} $$ Η ελαχιστοποίηση αυτής της απώλειας επικεντρώνεται στη διατήρηση της **τοπικής γεωμετρίας** και των σχέσεων μεταξύ των σημείων δεδομένων στον χώρο χαρακτηριστικών, μεταφέροντας μια δυνητικά πλουσιότερη μορφή γνώσης από τις απλές πιθανότητες εξόδου. Ο συγκεκριμένος πυρήνας που χρησιμοποιείται σε αυτή τη διατριβή είναι ο πυρήνας Ομοιότητας Συνημίτονου (Εξ. 4.3), ο οποίος προσφέρει πλεονεκτήματα όπως η μη απαίτηση ρητής ρύθμισης πλάτους ζώνης. Όταν ενσωματώνεται η KD σε ένα πλαίσιο DRL, ο συνολικός στόχος εκπαίδευσης συνδυάζει την τυπική απώλεια RL (π.χ., την απώλεια PPO $L_{RL}$) με την απώλεια απόσταξης. Η βασική δομή του συνδυασμένου κόστους, όπως προτάθηκε σε σχετική εργασία [22], είναι: $$ L = L_{RL} + \beta \cdot L_{dist} \quad \text{(Eq 3.12)} $$ Εδώ, το $L_{RL}$ αντιπροσωπεύει τον τυπικό στόχο DRL (π.χ., $L^{CLIP}(\theta)$ από την Εξ. 3.3), το $L_{dist}$ είναι η απώλεια αποσταγμάτωσης (η οποία μπορεί να είναι $L_{log}$, $L_{pkt}$, ή ένας συνδυασμός), και το $\beta$ είναι ένας παράγοντας βάρους που ελέγχει την επιρροή του όρου αποσταγμάτωσης.

[VISUALIZATION PLACEHOLDER: Type=Diagram, Title='Knowledge Distillation Overview', Data='Shows Teacher model and Student model. Arrow from Teacher output to Student loss (Logit Matching). Arrow from Teacher intermediate layer to Student loss (PKT). Combined loss function L = L_RL + beta * L_dist.']

## 3.3 Διαδικτυακή Απόσταξη Βασισμένη σε Συγκέντρωση  
Βασιζόμενη στις έννοιες της Πιθανοτικής Μεταφοράς Γνώσης (PKT) και της κλασικής αντιστοίχισης λογιστικής, η προτεινόμενη μέθοδος ενσωματώνει προσεγγίσεις **Ensemble KD** και **Online KD** για να ενισχύσει περαιτέρω τη σταθερότητα και την απόδοση του πράκτορα DRL.  
  
Το Ensemble KD αξιοποιεί ένα σύνολο από πολλαπλά μοντέλα δασκάλων για να παρέχει μια πιο ανθεκτική και ποικιλόμορφη πηγή γνώσης για τον μαθητή. Αυτοί οι δάσκαλοι μπορούν να αρχικοποιηθούν με διαφορετικούς τυχαίους σπόρους για να προωθήσουν την ποικιλία στις πολιτικές που έχουν μάθει. Η γνώση από αυτό το σύνολο συνδυάζεται με τον υπολογισμό του μέσου όρου των ατομικών απωλειών απόσταξης που υπολογίζονται μεταξύ του μαθητή και κάθε δασκάλου. Αν $N$ είναι ο αριθμός των επιλεγμένων καλύτερων δασκάλων και $L_{dist_i}$ είναι η απώλεια απόσταξης σε σχέση με τον δάσκαλο $i$, η συνολική απώλεια απόσταξης από το σύνολο, $L_{dist}$, υπολογίζεται ως εξής: $$ L_{dist} = \frac{1}{N} \sum_{i=1}^{N} L_{dist_i} \quad \text{(Eq 3.13)} $$ Αυτή η προσέγγιση εφαρμόζεται σε ένα περιβάλλον **Online KD**. Σε αντίθεση με την παραδοσιακή offline KD, όπου οι δάσκαλοι είναι πλήρως προεκπαιδευμένοι και στατικοί, η Online KD περιλαμβάνει την εκπαίδευση των δασκάλων και του μαθητή **ταυτόχρονα**. Αυτή η δυναμική προσέγγιση είναι ιδιαίτερα κατάλληλη για ευμετάβλητα και μη σταθερά περιβάλλοντα όπως οι χρηματοπιστωτικές αγορές, καθώς οι δάσκαλοι μπορούν δυνητικά να προσαρμοστούν παράλληλα με τον μαθητή. 

Επιπλέον, η Online KD αποφεύγει την υπολογιστική πολυπλοκότητα και την επιπλέον ρύθμιση υπερπαραμέτρων που σχετίζεται με τη χωριστή, διφασική διαδικασία εκπαίδευσης που απαιτείται για την offline KD. Η συγκεκριμένη διαδικασία εκπαίδευσης online ensemble που χρησιμοποιείται σε αυτή τη διατριβή είναι η εξής: Αρχικά, ένα σύνολο μοντέλων δασκάλων και το μοντέλο μαθητή εκπαιδεύονται ταυτόχρονα χρησιμοποιώντας τις ίδιες παρτίδες δεδομένων από το περιβάλλον (αναφερόμενοι στη γενική δομή που φαίνεται στο Σχήμα 3.1). Μετά από μια αρχική περίοδο εκπαίδευσης (π.χ. τις πρώτες 50 εποχές), η απόδοση όλων των μοντέλων δασκάλων αξιολογείται με βάση ένα σχετικό μέτρο, όπως το σωρευτικό κέρδος και ζημία (PnL) της εκπαίδευσής τους.

Οι **καλύτεροι $N$ δάσκαλοι** επιλέγονται στη συνέχεια από το αρχικό σύνολο. Η ταυτόχρονη διαδικασία εκπαίδευσης συνεχίζεται στη συνέχεια μόνο με αυτούς τους επιλεγμένους $N$ δασκάλους και το μοντέλο του μαθητή. Αυτό διασφαλίζει ότι ο μαθητής καθοδηγείται κυρίως από τις πιο επιτυχημένες πολιτικές των δασκάλων που έχουν διδαχθεί κατά την αρχική φάση. Ο τελικός εκπαιδευτικός στόχος για τον πράκτορα DRL του μαθητή συνδυάζει την τυπική απώλεια DRL ($L_{RL}$), την απώλεια αντιστοίχισης λογιτών ($L_{log}$) και την απώλεια PKT ($L_{pkt}$), ενσωματώνοντας τη μέση τιμή συνόλου για τους όρους απόσταξης. Η **τελική συνδυασμένη συνάρτηση απώλειας** δομείται ως εξής: $$ L = L_{RL} + \beta_1 \cdot L_{log} + \beta_2 \cdot L_{pkt} \quad \text{(Eq 3.14)} $$ όπου το $L_{RL}$ είναι ο αντικειμενικός στόχος PPO clipped surrogate (Eq 3.3), το $L_{log}$ είναι η μέση απώλεια αντιστοίχισης λογιτών από το σύνολο δασκάλων (Eq 3.8), και το $L_{pkt}$ είναι η μέση απώλεια PKT από το σύνολο δασκάλων (Eq 3.11).  Οι παράγοντες βάρους $\beta_1$ και $\beta_2$ ελέγχουν τη συμβολή κάθε όρου απόσταξης στη συνολική απώλεια. Σε αυτή τη μελέτη, οι τυπικοί συντελεστές βαρύτητας ορίζονται σε $\beta_1 = 1$ και $\beta_2 = 1$, δίνοντας ίση σημασία και στους δύο τύπους απόσταξης. Αυτό το συνδυασμένο διαδικτυακό πλαίσιο αποσταγμένης σύνθεσης αποσκοπεί στην παροχή ενός σταθερού και αποτελεσματικού σήματος μάθησης στον φοιτητή πράκτορα DRL στο απαιτητικό περιβάλλον χρηματοοικονομικών συναλλαγών.  
  
## 3.4 Αρχιτεκτονική Δικτύου  
  
Η αρχιτεκτονική του νευρωνικού δικτύου που χρησιμοποιείται και για τα μοντέλα δασκάλου και μαθητή σε αυτό το πλαίσιο είναι ταυτόσημη. Αυτή η κοινή αρχιτεκτονική είναι σχεδιασμένη να επεξεργάζεται τα διαδοχικά, μηχανικά χαρακτηριστικά τιμής και τις πληροφορίες θέσης του πράκτορα για να εξάγει πιθανότητες δράσης για τον ηθοποιό και εκτιμήσεις αξίας κατάστασης για τον κριτικό. Η αρχιτεκτονική βασίζεται στο σχέδιο που περιγράφεται σε σχετική εργασία (Moustakidis Section 4.1.3 context). Η δομή του δικτύου αποτελείται από τα εξής επίπεδα: * **Επίπεδο Εισόδου:** Αυτή η στρώση λαμβάνει τα μηχανικά χαρακτηριστικά που βασίζονται στην τιμή, όπως ορίζονται και προεπεξεργάζονται στην Ενότητα 3.2.1 (αναφερόμενη στη μεθοδολογία παρόμοια με το πλαίσιο της Ενότητας 4.1.2 του Moustakidis). Αυτά τα χαρακτηριστικά καταγράφουν σχετικές πληροφορίες από τα ιστορικά δεδομένα OHLCV. * **Στρώμα LSTM:** Ένα στρώμα Long Short-Term Memory χρησιμοποιείται για την επεξεργασία των διαδοχικών εισερχόμενων δεδομένων και την καταγραφή των χρονικών εξαρτήσεων. Αυτό το επίπεδο αποτελείται από **32 κρυφές μονάδες**. * **Επίπεδο Κωδικοποίησης Θέσης:** Η προηγούμενη θέση του πράκτορα (αναπαριστώμενη ως ένα διανύσμα 3 τιμών που υποδεικνύει μακροχρόνια, βραχυχρόνια ή ουδέτερη) κωδικοποιείται σε ένα διανύσμα διαστάσεων **10**. Αυτός ο κωδικοποιημένος διανύσματος θέσης στη συνέχεια συγχωνεύεται με την έξοδο του επιπέδου LSTM. * **Βασικό Δίκτυο:** Η συγχωνευμένη έξοδος από τα στρώματα LSTM και κωδικοποίησης θέσης τροφοδοτείται σε μια βασική δομή δικτύου. Αυτό συνήθως αποτελείται από ένα **Γραμμικό επίπεδο** που ακολουθείται από μια **Συνάρτηση ενεργοποίησης Sigmoid Linear Unit (SiLU)** και ένα **Επίπεδο Dropout** που εφαρμόζεται στην έξοδο του για κανονικοποίηση. * **Κεφαλή Ηθοποιού:** Ένα ξεχωριστό **Γραμμικό επίπεδο** παίρνει την έξοδο του βασικού δικτύου και την μετατρέπει σε **3 τιμές**, που αντιπροσωπεύουν τις πιθανότητες ή τα logits για τις πιθανές διακριτές ενέργειες του πράκτορα: αγορά, κράτημα και πώληση. * **Κεφαλή Κριτικού:** Μια άλλη ξεχωριστή **Γραμμική στρώση** παίρνει την έξοδο του βασικού δικτύου και την προβάλλει σε μια **μοναδική τιμή**, που αντιπροσωπεύει την εκτίμηση του δικτύου για την τιμή της κατάστασης $V(s)$. Οι προκαταλήψεις περιλαμβάνονται σε όλα τα γραμμικά επίπεδα της αρχιτεκτονικής του δικτύου, με εξαίρεση το γραμμικό επίπεδο που χρησιμοποιείται ειδικά για την κωδικοποίηση θέσης. Μια οπτική αναπαράσταση που απεικονίζει αυτή την αρχιτεκτονική Actor-Critic μπορεί να βρεθεί στην Εικόνα 4.2 του πρωτότυπου κειμένου της διατριβής (πηγή Moustakidis).


## Κεφάλαιο 4: Πειραματική Αξιολόγηση  
  
Αυτό το κεφάλαιο παρουσιάζει την εμπειρική αξιολόγηση που πραγματοποιήθηκε για να εκτιμηθεί η αποτελεσματικότητα της προτεινόμενης μεθοδολογίας. Βασιζόμενοι στη θεωρητική θεμελίωση και τον τεχνικό σχεδιασμό που αναλύονται στα προηγούμενα κεφάλαια, αυτό το κεφάλαιο περιγράφει την πειραματική ρύθμιση, αναλύει τα σύνολα δεδομένων και τα χαρακτηριστικά που χρησιμοποιήθηκαν, καθορίζει τις βασικές μεθόδους για σύγκριση και προσδιορίζει τα μετρικά που χρησιμοποιούνται για την ποσοτικοποίηση της απόδοσης και της σταθερότητας. Η εμπειρική αξιολόγηση είναι ένα κρίσιμο συστατικό αυτής της διατριβής, καθώς παρέχει τα απαραίτητα στοιχεία για την επικύρωση της αποτελεσματικότητας της ενσωμάτωσης προηγμένων τεχνικών Απόσταξης Γνώσης (KD) στο πλαίσιο της Βαθιάς Ενίσχυσης Μάθησης (DRL) για την αντιμετώπιση των προκλήσεων του εμπορίου κρυπτονομισμάτων. Τα πειράματα έχουν σχεδιαστεί για να δοκιμάσουν αυστηρά τις υποθέσεις και να απαντήσουν στις ερευνητικές ερωτήσεις που τέθηκαν στο Κεφάλαιο 1. Οι επόμενες ενότητες αυτού του κεφαλαίου θα καλύψουν τις εξής βασικές περιοχές:  
* ** 4.1 Πειραματική Ρύθμιση:** 4.1 Πειραματική Ρύθμιση:** Αυτή η ενότητα θα αναλύσει το συνολικό πειραματικό σχέδιο, συμπεριλαμβανομένης της διαμόρφωσης του εμπορικού περιβάλλοντος, των λεπτομερειών της ρύθμισης του πράκτορα DRL (PPO) και των λεπτομερειών υλοποίησης των ενσωματωμένων τεχνικών KD. Πληροφορίες σχετικά με το υλικό και το λογισμικό που χρησιμοποιήθηκαν θα παρασχεθούν επίσης.  
Πλαίσιο: Πληροφορίες σχετικά με το υλικό και το λογισμικό που χρησιμοποιήθηκαν θα παρασχεθούν επίσης. * ** 4.2 Περιγραφή Δεδομένων:** 4.2 Περιγραφή Δεδομένων:** Αυτή η ενότητα θα παρέχει μια λεπτομερή περιγραφή του συγκεκριμένου συνόλου δεδομένων κρυπτονομισμάτων που χρησιμοποιήθηκε για την εκπαίδευση και την αξιολόγηση, συμπεριλαμβανομένης της χρονικής περιόδου που καλύπτεται, της συχνότητας των δεδομένων και της διαδικασίας μηχανικής χαρακτηριστικών βάσει τιμών.  
* ** 4.3 Βασικές Μέθοδοι:** Αυτή η ενότητα θα καθορίσει τις εναλλακτικές προσεγγίσεις και μοντέλα με τα οποία συγκρίνεται το προτεινόμενο πλαίσιο DRL/προηγμένης KD, όπως οι πράκτορες DRL που εκπαιδεύονται χωρίς KD ή με τις τυπικές μεθόδους KD.  
* ** 4.4 Μετρικές Αξιολόγησης:** Αυτή η ενότητα θα καθορίσει τα ποσοτικά μετρικά που χρησιμοποιούνται για την αξιολόγηση της απόδοσης των εμπορικών πρακτόρων, εστιάζοντας σε μέτρα κερδοφορίας, προσαρμοσμένης στον κίνδυνο απόδοσης και σταθερότητας εκπαίδευσης, καθώς και σε μεθόδους για την αξιολόγηση της στατιστικής σημασίας.  
* ** 4.5 Λεπτομέρειες Υλοποίησης:** Αυτή η ενότητα θα παρέχει περαιτέρω συγκεκριμένες λεπτομέρειες σχετικά με τις βιβλιοθήκες λογισμικού, τις ρυθμίσεις υπερπαραμέτρων και οποιαδήποτε άλλη τεχνική πτυχή κρίσιμη για την αναπαραγωγιμότητα των πειραμάτων, ιδιαίτερα όσον αφορά την υλοποίηση PKT και online ensemble. Συλλογικά, αυτό το κεφάλαιο θέτει τα θεμέλια για την παρουσίαση και συζήτηση των εμπειρικών αποτελεσμάτων και των επιπτώσεών τους στο Κεφάλαιο 5.  
### 4.1.1 Διαμόρφωση Εμπορικού Περιβάλλοντος  
  
Αυτή η ενότητα περιγράφει τη ρύθμιση και τη διαμόρφωση του προσομοιωμένου περιβάλλοντος συναλλαγών κρυπτονομισμάτων που χρησιμοποιείται για την εκπαίδευση και την αξιολόγηση των πρακτόρων Βαθιάς Ενίσχυσης Μάθησης (DRL). Το περιβάλλον έχει σχεδιαστεί για να μιμείται τις βασικές δυναμικές και χαρακτηριστικά των πραγματικών αγορών κρυπτονομισμάτων με βάση ιστορικά δεδομένα.  
**Παράμετροι Προσομοίωσης:** Οι αλληλεπιδράσεις της αγοράς εντός του περιβάλλοντος προσομοιώνονται χρησιμοποιώντας ιστορικά δεδομένα τιμών κρυπτονομισμάτων. Οι εκτελέσεις παραγγελιών υποτίθεται ότι πραγματοποιούνται στην ζητούμενη τιμή (ή στην επόμενη διαθέσιμη τιμή σε διακριτά χρονικά βήματα) χωρίς σημαντική επίδραση στην αγορά, αν και η μοντελοποίηση πιο σύνθετης δυναμικής βιβλίου παραγγελιών ή ολίσθησης θα μπορούσε να εξεταστεί σε προχωρημένες ρυθμίσεις. Η προσομοίωση ξεκινά με έναν καθορισμένο **αρχικό κεφάλαιο** που έχει κατανεμηθεί στον πράκτορα.  
**Χώρος Δράσης:** Ο **χώρος δράσης** του πράκτορα καθορίζει το σύνολο των διαθέσιμων αποφάσεων συναλλαγών σε κάθε χρονικό βήμα. Για αυτήν την έρευνα, ο πράκτορας είναι διαμορφωμένος να διαπραγματεύεται ένα συγκεκριμένο σύνολο κρυπτονομισματικών περιουσιακών στοιχείων. Οι πιθανές ενέργειες για κάθε περιουσιακό στοιχείο περιλαμβάνουν συνήθως διακριτές επιλογές όπως **Αγορά**, **Πώληση** ή **Διατήρηση** (διατήρηση της τρέχουσας θέσης). Η κλίμακα αυτών των ενεργειών μπορεί να είναι σταθερή (π.χ., να πραγματοποιηθεί συναλλαγή με σταθερό μέγεθος παρτίδας) ή μεταβλητή (π.χ., να καθοριστεί ένα ποσοστό του διαθέσιμου κεφαλαίου προς κατανομή), ανάλογα με τη συγκεκριμένη διατύπωση του προβλήματος.  
**Αναπαράσταση Κατάστασης:** Η παρατήρηση του πράκτορα, ή **αναπαράσταση κατάστασης**, σε κάθε χρονικό βήμα είναι κρίσιμη για την ενημερωμένη λήψη αποφάσεων. Η κατάσταση αποτελείται από ένα σύνολο χαρακτηριστικών που έχουν σχεδιαστεί για να παρέχουν στον πράκτορα σχετικές πληροφορίες της αγοράς. Αυτό περιλαμβάνει τα **8 μηχανικά χαρακτηριστικά βασισμένα στην τιμή** που προέρχονται από ιστορικά δεδομένα OHLCV (Άνοιγμα, Υψηλό, Χαμηλό, Κλείσιμο, Όγκος), όπως περιγράφεται στην Ενότητα 3.2.1 (αναφορά στη μεθοδολογία παρόμοια με το πλαίσιο Moustakidis 4.1.2). Αυτές οι παράμετροι καταγράφουν τις τάσεις, την αστάθεια και τις σχετικές μεταβολές τιμών. Επιπλέον, η κατάσταση περιλαμβάνει **πληροφορίες χαρτοφυλακίου**, όπως το τρέχον υπόλοιπο μετρητών του πράκτορα και την ποσότητα κάθε περιουσιακού στοιχείου που κατέχεται. Δεδομένης της χρήσης LSTMs στην αρχιτεκτονική του δικτύου (Ενότητα 3.4), η αναπαράσταση της κατάστασης περιλαμβάνει ένα **παράθυρο αναδρομής** αυτών των χαρακτηριστικών σε πρόσφατα χρονικά βήματα για να συλλάβει το χρονικό πλαίσιο.  
  
**Κόστη Συναλλαγών:** Για να αντικατοπτρίζονται οι συνθήκες πραγματικού κόσμου, τα **κόστη συναλλαγών** μοντελοποιούνται. Εφαρμόζεται μια σταθερή **ποσοστιαία προμήθεια** ανά συναλλαγή, συνήθως ένα μικρό ποσοστό της αξίας της συναλλαγής (π.χ., 2e-5). Για τους σκοπούς αυτής της έρευνας, άλλα πιθανά κόστη όπως η ολίσθηση, η οποία μπορεί να προκύψει λόγω της μεταβλητότητας της αγοράς και του μεγέθους της παραγγελίας που επηρεάζει την πραγματική τιμή εκτέλεσης, δεν μοντελοποιούνται ρητά εκτός αν αναφέρεται διαφορετικά, εστιάζοντας κυρίως στην επίδραση των προμηθειών.  
  
**Συνάρτηση Ανταμοιβής:** Η **συνάρτηση ανταμοιβής** είναι κεντρική για την καθοδήγηση της διαδικασίας μάθησης του πράκτορα DRL, παρέχοντας ένα σκαλάρ σήμα σε κάθε χρονικό βήμα με βάση το αποτέλεσμα των ενεργειών του πράκτορα. Η ανταμοιβή υπολογίζεται συνήθως με βάση την **αλλαγή στην αξία του χαρτοφυλακίου του πράκτορα** που προκύπτει από τις κινήσεις των τιμών και τις εκτελεσμένες συναλλαγές, αφού ληφθούν υπόψη τα κόστη συναλλαγών. Εναλλακτικά, μπορεί να αντιπροσωπεύει το **πραγματοποιηθέν PnL** από κλειστές θέσεις ή να περιλαμβάνει στοιχεία από μια **συγκεκριμένη διαμορφωμένη συνάρτηση ανταμοιβής** που έχει σχεδιαστεί για να ενθαρρύνει τις επιθυμητές εμπορικές συμπεριφορές ή τη διαχείριση κινδύνου, όπως συζητείται στη μεθοδολογία (Ενότητα 3.1).  
  
### 4.1.2 Λεπτομέρειες Υλοποίησης PPO  
Αυτή η ενότητα περιγράφει τις συγκεκριμένες λεπτομέρειες υλοποίησης και τις ρυθμίσεις υπερπαραμέτρων που χρησιμοποιούνται για τον αλγόριθμο Proximal Policy Optimization (PPO), ο οποίος αποτελεί τον πυρήνα της διαδικασίας εκπαίδευσης του πράκτορα DRL. Ο πράκτορας DRL χρησιμοποιεί την προηγουμένως περιγραφόμενη **αρχιτεκτονική δικτύου** (Ενότητα 3.4), βασισμένη στο σχέδιο στο πλαίσιο της Ενότητας 4.1.3 του Μουστακίδη, τόσο για τα συστατικά του Actor όσο και του Critic. Αυτή η αρχιτεκτονική είναι υπεύθυνη για την επεξεργασία των παρατηρήσεων κατάστασης και την εξαγωγή πιθανοτήτων δράσης και εκτιμήσεων τιμής κατάστασης. Η διαδικασία εκπαίδευσης ρυθμίζεται χρησιμοποιώντας ένα συγκεκριμένο σύνολο **υπερπαραμέτρων**:  
* **Ρυθμός Μάθησης:** Ο ρυθμός μάθησης για τον βελτιστοποιητή έχει οριστεί σε μια τιμή όπως **5e-4**.  
* ** Μέγεθος Παρτίδας:** Οι ενημερώσεις πολιτικής και αξίας πραγματοποιούνται χρησιμοποιώντας μίνι-παρτίδες εμπειρίας που δειγματίζονται από τη μνήμη επαναπαραγωγής, με μέγεθος παρτίδας 32. * **Βελτιστοποιητής:** Ο βελτιστοποιητής **RAdam** χρησιμοποιείται για την ενημέρωση των παραμέτρων του δικτύου.  
* **Περικοπή PPO ($\epsilon$):** Η παράμετρος περικοπής $\epsilon$ για τον περικομμένο υποκατάστατο στόχο PPO (Εξ. 3.3) έχει οριστεί σε **0.2**. * **Παράμετροι GAE:** Ο υπολογισμός της Γενικευμένης Εκτίμησης Πλεονεκτήματος (GAE) (Εξ. 3.5) χρησιμοποιεί τον παράγοντα έκπτωσης **$\gamma$** και την παράμετρο GAE **$\lambda$**.  
* **Παράμετροι GAE:** Ο υπολογισμός της Γενικευμένης Εκτίμησης Πλεονεκτήματος (GAE) (Εξ. 3.5) χρησιμοποιεί τον παράγοντα έκπτωσης **$\gamma$** και την παράμετρο GAE **$\lambda$**. * **Εποχές ανά Παρτίδα:** Εποχές ανά Παρτίδα:** Για κάθε παρτίδα εμπειρίας που δειγματοληπτείται, ο αλγόριθμος εκτελεί πολλαπλές εποχές ενημερώσεων βελτιστοποίησης στις συναρτήσεις πολιτικής και αξίας. Ο **αριθμός των εποχών** για τις ενημερώσεις πολιτικής/τιμής ανά παρτίδα δεδομένων καθορίζεται.  
* **Συντελεστής Εντροπίας:** Ένας **συντελεστής εντροπίας** μπορεί να συμπεριληφθεί στη συνάρτηση απώλειας για να ενθαρρύνει την εξερεύνηση κατά την εκπαίδευση, προσθέτοντας έναν όρο ανάλογο με την εντροπία της κατανομής πολιτικής. Διατηρείται μια **μνήμη επανάληψης εμπειρίας** για να αποθηκεύει τα δεδομένα που συλλέγονται από τις αλληλεπιδράσεις του πράκτορα με το περιβάλλον. Αυτό το buffer αποθηκεύει στοιχεία όπως παρατηρημένες καταστάσεις, εκτελεσμένες ενέργειες, ληφθείσες ανταμοιβές, επόμενες καταστάσεις και σημαίες τερματισμού. Το **μέγεθος** αυτού του αποθηκευτικού χώρου μνήμης έχει ρυθμιστεί ώστε να αποθηκεύει μια επαρκή ποσότητα πρόσφατης εμπειρίας. Για τις ενημερώσεις εκπαίδευσης, **παρτίδες εμπειρίας** επιλέγονται τυχαία από αυτό το μπουφέ μνήμης. Αυτά τα δειγματοληπτικά δεδομένα χρησιμοποιούνται στη συνέχεια για τον υπολογισμό των συναρτήσεων απώλειας και των κλίσεων για τα δίκτυα Actor και Critic. Η συνολική διαδικασία **ενημέρωσης PPO** ακολουθεί έναν κύκλο: Ο πράκτορας αλληλεπιδρά με το περιβάλλον για να συλλέξει μια παρτίδα εμπειρίας. Τα πλεονεκτήματα υπολογίζονται για αυτή τη δόση χρησιμοποιώντας τη μέθοδο GAE. Η συλλεγμένη εμπειρία αποθηκεύεται στη μνήμη επανάληψης. Μίνι-παρτίδες δειγματοληπτούνται από τη μνήμη. Πολλαπλοί εποχές ενημερώσεων βελτιστοποίησης εκτελούνται στα δίκτυα Actor και Critic χρησιμοποιώντας τα δειγματισμένα mini-batches και τη συνδυασμένη συνάρτηση απώλειας (ενσωματώνοντας τις απώλειες KD, αν είναι εφαρμόσιμο). Αυτός ο κύκλος συλλογής δεδομένων και βελτιστοποίησης επαναλαμβάνεται καθ' όλη τη διάρκεια της εκπαίδευσης.  
### 4.1.3 Υλοποίηση Απόσταξης Γνώσης  
Αυτή η ενότητα περιγράφει τις συγκεκριμένες επιλογές υλοποίησης που έγιναν για τις τεχνικές Απόσταξης Γνώσης (KD) που ενσωματώθηκαν στο πλαίσιο της Βαθιάς Ενίσχυσης Μάθησης (DRL). Αυτές οι τεχνικές, συμπεριλαμβανομένου του τυπικού ταυτοτισμού λογιών, της Πιθανοτικής Μετάδοσης Γνώσης (PKT), των συνόλων δασκάλων και της διαδικτυακής απόσταξης, εφαρμόζονται για τη μεταφορά της αποκτηθείσας γνώσης από ένα σύνολο μοντέλων δασκάλων στον μαθητή πράκτορα με στόχο τη βελτίωση της σταθερότητας εκπαίδευσης και της απόδοσης συναλλαγών.  
  
**Standard KD (Logit Matching):** Η κλασική μέθοδος KD με λογιστική αντιστοίχιση εφαρμόζεται για τη μεταφορά γνώσης από το επίπεδο εξόδου των δικτύων Actor των μοντέλων δασκάλων. Αυτή η μέθοδος βασίζεται στη δημιουργία **μαλακών κατανομών πιθανοτήτων** πάνω στον χώρο δράσης. Ο βαθμός μαλακότητας ελέγχεται από μια καθορισμένη **θερμοκρασία $T$**. Η απώλεια απόσταξης για την αντιστοίχιση λογιτών, $L_{log}$, υπολογίζεται ως η απόκλιση Kullback-Leibler (KL) μεταξύ των μαλακών πιθανοτήτων του μαθητή και του συνόλου των δασκάλων (Εξ. 3.8). Αυτή η απώλεια ενθαρρύνει τις προτιμήσεις δράσης και τα επίπεδα εμπιστοσύνης του μαθητή να ευθυγραμμίζονται με αυτά των δασκάλων.  
**Πιθανιστική Μεταφορά Γνώσης (PKT):** Για τη μεταφορά γνώσης από τις εσωτερικές αναπαραστάσεις που έχουν μάθει τα μοντέλα δασκάλων, εφαρμόζεται η Μεταφορά Πιθανοτήτων Γνώσης (PKT). Αυτή η μέθοδος λειτουργεί σε ένα συγκεκριμένο **ενδιάμεσο στρώμα** εντός της αρχιτεκτονικής του δικτύου (όπως αναλύεται στην Ενότητα 3.4). Τα διανύσματα χαρακτηριστικών εξάγονται από αυτό το επιλεγμένο στρώμα και για τα δύο μοντέλα, του δασκάλου και του μαθητή. Η ομοιότητα μεταξύ αυτών των διανυσμάτων χαρακτηριστικών για ζεύγη σημείων δεδομένων μετράται χρησιμοποιώντας τον πυρήνα **Cosine Similarity** (Εξ. 4.3), ο οποίος επιλέγεται για την αποτελεσματικότητά του και την απουσία υπερπαραμέτρων όπως το πλάτος ζώνης. Η απώλεια PKT, $L_{pkt}$, ελαχιστοποιεί την KL απόκλιση μεταξύ των κατανομών πιθανότητας υπό συνθήκες που προκύπτουν από αυτές τις ομοιότητες χαρακτηριστικών (Εξ. 3.11). Αυτή η διαδικασία στοχεύει στη μεταφορά των δομικών σχέσεων και της τοπικής γεωμετρίας του χώρου χαρακτηριστικών που έχει μάθει ο δάσκαλος στον μαθητή.  
  
**Σύνολο Δασκάλων:** Η γνώση αποστάζεται από ένα **σύνολο δασκάλων**, που αποτελείται από έναν καθορισμένο αριθμό μοντέλων δασκάλων που αρχικοποιούνται με διαφορετικούς τυχαίους σπόρους. Μετά από μια αρχική φάση εκπαίδευσης, συγκεκριμένα σε μια καθορισμένη **εποχή επιλογής** (π.χ., εποχή 50), η απόδοση κάθε μοντέλου δασκάλου αξιολογείται χρησιμοποιώντας ένα σχετικό **μετρικό επιλογής** (π.χ., σωρευτικό PnL εκπαίδευσης). Οι **καλύτεροι $N$ δάσκαλοι** επιλέγονται από την αρχική δεξαμενή με βάση αυτό το μέτρο. Κατά τη διάρκεια της επόμενης εκπαίδευσης, η γνώση από αυτό το επιλεγμένο σύνολο συγκεντρώνεται με το **μέσο όρο των ατομικών απωλειών απόσταξης τους** (Εξ. 3.13) για τους όρους αντιστοίχισης λογιστικών ($L_{log}$) και PKT ($L_{pkt}$).  
**Online Απόσταξη:** Η διαδικασία εκπαίδευσης, που περιλαμβάνει τόσο το επιλεγμένο σύνολο δασκάλων όσο και το μοντέλο μαθητή, διεξάγεται ως **Online Απόσταξη**. Αυτή η προσέγγιση περιλαμβάνει την ταυτόχρονη εκπαίδευση των δασκάλων και του μαθητή. Και τα δύο σύνολα μοντέλων λαμβάνουν τις ίδιες παρτίδες δεδομένων που δειγματίζονται από το περιβάλλον κατά τη διάρκεια κάθε επανάληψης εκπαίδευσης, επιτρέποντας τη δυναμική μεταφορά γνώσης και πιθανώς καλύτερη προσαρμογή στο μη σταθερό χρηματοοικονομικό περιβάλλον σε σύγκριση με τις offline μεθόδους.  
**Συνδυασμένη Απώλεια:** Η βελτιστοποίηση των παραμέτρων του μαθητευόμενου πράκτορα καθοδηγείται από την ελαχιστοποίηση μιας **τελικής συνδυασμένης συνάρτησης απώλειας**. Αυτή η συνάρτηση ενσωματώνει την τυπική απώλεια DRL ($L_{RL}$) με τις συγκεντρωμένες απώλειες απόσταξης από το σύνολο δασκάλων ($L_{log}$ και $L_{pkt}$). Η συνδυασμένη απώλεια ορίζεται επίσημα ως: $$ L = L_{RL} + \beta_1 \cdot L_{log} + \beta_2 \cdot L_{pkt} \quad \text{(Eq 3.14)} $$ Εδώ, το $L_{RL}$ αντιπροσωπεύει τον στόχο PPO clipped surrogate (Εξ. 3.3).  $L_{log}$ είναι η μέση απώλεια αντιστοίχισης λογιστικών από την επιλεγμένη ομάδα δασκάλων (Εξ. 3.8), και $L_{pkt}$ είναι η μέση απώλεια PKT από την επιλεγμένη ομάδα δασκάλων (Εξ. 3.11). $\beta_1$ και $\beta_2$ είναι υπερπαράμετροι βάρους που ελέγχουν τη σχετική συμβολή των όρων αντιστοίχισης λογιστικών και απόσταξης PKT, αντίστοιχα. Σε αυτή την υλοποίηση, **$\beta_1$ και $\beta_2$ έχουν οριστεί στο 1**, δίνοντας ίση βαρύτητα και στις δύο μορφές απόσταξης. Αυτός ο συνδυασμένος στόχος καθοδηγεί τις ενημερώσεις παραμέτρων του μαθητή, επιτρέποντάς του να μάθει μια αποτελεσματική πολιτική συναλλαγών ενώ επωφελείται από τη δομημένη γνώση που μεταφέρεται από το σύνολο δασκάλων και στα δύο επίπεδα εξόδου και ενδιάμεσα με διαδικτυακό τρόπο.  
  
### 4.2.1 Πηγές Δεδομένων Κρυπτονομισμάτων  
Αυτή η ενότητα περιγράφει την προέλευση και τα κύρια χαρακτηριστικά των δεδομένων της αγοράς κρυπτονομισμάτων που χρησιμοποιούνται για την εκπαίδευση και την αξιολόγηση των πρακτόρων Βαθιάς Ενισχυτικής Μάθησης σε αυτή τη διατριβή. Η επιλογή του συνόλου δεδομένων είναι κρίσιμη για να διασφαλιστεί ότι τα πειραματικά αποτελέσματα είναι αντιπροσωπευτικά των προκλήσεων που υπάρχουν στις ασταθείς αγορές κρυπτονομισμάτων. Το σύνολο δεδομένων περιλαμβάνει **17 ζεύγη κρυπτονομισμάτων προς σταθερό νόμισμα**, συμπεριλαμβανομένων των σημαντικών ζευγών όπως BTC/USDT. Αυτό το εκτενές σύνολο δεδομένων παρέχεται από την SpeedLab AG. Τα ακατέργαστα δεδομένα αποτελούνται από λεπτό προς λεπτό αρχεία OHLCV (Άνοιγμα, Υψηλό, Χαμηλό, Κλείσιμο, Όγκος). Η πλήρης χρονική περίοδος που καλύπτεται από το σύνολο δεδομένων εκτείνεται από **17 Αυγούστου 2017, στις 5 π.μ. έως 2 Φεβρουαρίου 2022, στις 6 π.μ.** Το πλήρες χρονικό διάστημα που καλύπτεται από το σύνολο δεδομένων εκτείνεται από **17 Αυγούστου 2017, στις 5 π.μ. έως 2 Φεβρουαρίου 2022, στις 6 π.μ.** Ενώ τα ακατέργαστα δεδομένα είναι σε επίπεδο λεπτού, για τους σκοπούς των πειραμάτων που διεξήχθησαν σε αυτή τη διατριβή, τα δεδομένα **δειγματίστηκαν σε ωριαία βάση**. Ενώ τα ακατέργαστα δεδομένα είναι σε επίπεδο λεπτού, για τους σκοπούς των πειραμάτων που διεξήχθησαν σε αυτή τη διατριβή, τα δεδομένα **δειγματίστηκαν σε ωριαία βάση**. Αυτή η ωριαία συχνότητα παρέχει μια κατάλληλη ισορροπία μεταξύ της καταγραφής επαρκούς δυναμικής τιμών και της διαχείρισης της υπολογιστικής πολυπλοκότητας για την εκπαίδευση DRL. Το σύνολο δεδομένων χωρίστηκε σε διακριτές περιόδους εκπαίδευσης και δοκιμής για να διευκολυνθεί η σωστή αξιολόγηση του μοντέλου σε μη ορατά δεδομένα. Το **σύνολο εκπαίδευσης ολοκληρώνεται στις 5 Σεπτεμβρίου 2021, στις 11 μ.μ.** Το **σύνολο εκπαίδευσης ολοκληρώνεται στις 5 Σεπτεμβρίου 2021, στις 11 μ.μ.** Τα υπόλοιπα δεδομένα, από τις 5 Σεπτεμβρίου 2021, 11 μ.μ. και μετά έως το τέλος του συνόλου δεδομένων (2 Φεβρουαρίου 2022, 6 π.μ.), αποτελούν το σύνολο δοκιμών. Τα υπόλοιπα δεδομένα, από τις 5 Σεπτεμβρίου 2021, 11 μ.μ. και μετά έως το τέλος του συνόλου δεδομένων (2 Φεβρουαρίου 2022, 6 π.μ.), αποτελούν το σύνολο δοκιμών. Αυτή η διάκριση επιτρέπει μια αμερόληπτη αξιολόγηση της απόδοσης του εκπαιδευμένου πράκτορα σε μια ξεχωριστή χρονική περίοδο.


### 4.2.2 Διαδικασία Feature Engineering 
Αυτή η ενότητα περιγράφει τη διαδικασία μηχανικής χαρακτηριστικών από τα ακατέργαστα δεδομένα OHLCV που θα χρησιμοποιηθούν ως είσοδος για τα μοντέλα Βαθιάς Ενίσχυσης Μάθησης. Η λογική πίσω από αυτήν την προσέγγιση είναι να εξάγουμε πιο σημαντικά σήματα από τις ακατέργαστες τιμές και τον όγκο και να δημιουργήσουμε χαρακτηριστικά που να μοιάζουν με τις τεχνικές ανάλυσης που χρησιμοποιούν συνήθως οι ανθρώπινοι έμποροι. Η είσοδος για αυτή τη διαδικασία είναι τα ακατέργαστα **δεδομένα OHLCV** για κάθε κρυπτονόμισμα σε κάθε χρονική στιγμή. Από αυτά τα δεδομένα, προκύπτουν συνολικά 8 μηχανικά χαρακτηριστικά. Έξι από αυτά τα χαρακτηριστικά βασίζονται σε **Ποσοστά Διαφορών Κεριών Υψηλού/Χαμηλού/Κλεισίματος** Με ένα παράθυρο εξομάλυνσης 10 βημάτων**: Αυτές οι δυνατότητες καταγράφουν τη σχετική θέση των τρεχουσών τιμών Υψηλού, Χαμηλού και Κλεισίματος σε σύγκριση με τους λειασμένους μέσους όρους τους κατά τα προηγούμενα 10 χρονικά βήματα. Οι υπολογισμοί πραγματοποιούνται ως εξής:  

  
Αυτές οι τρεις υπολογισμοί εφαρμόζονται χρησιμοποιώντας διαφορετικές "μετατοπισμένες τιμές" για να δημιουργήσουν συνολικά έξι χαρακτηριστικά σε αυτή την κατηγορία, καταγράφοντας τις παραλλαγές στις πρόσφατες τιμές. Τα υπόλοιπα δύο χαρακτηριστικά προέρχονται από τις αναλογίες τιμών **High/Low To Previous Close**:  
Αυτές οι δυνατότητες αντιπροσωπεύουν την ποσοστιαία μεταβολή των τρεχουσών τιμών Υψηλού και Χαμηλού σε σχέση με την τιμή Κλεισίματος του αμέσως προηγούμενου χρονικού βήματος. Οι υπολογισμοί είναι:  

* For the High price: $\frac{mean(t)-p_h(t)}{mean(t)}$, where $$ mean(t)=\frac{1}{10}\sum_{i=1}^{10}p_h(t-i) $$ 
* For the Close price: $\frac{mean(t)-p_c(t)}{mean(t)}$, where $$ mean(t)=\frac{1}{10}\sum_{i=1}^{10}p_c(t-i) $$
* For the Low price: $\frac{mean(t)-p_l(t)}{mean(t)}$, where $$\ mean(t)=\frac{1}{10}\sum_{i=1}^{10}p_l(t-i) $$ 
Αυτές οι χαρακτηριστικές τιμές αντιπροσωπεύουν την ποσοστιαία μεταβολή των τρεχουσών τιμών Υψηλού και Χαμηλού σε σχέση με την τιμή Κλεισίματος του αμέσως προηγούμενου χρονικού βήματος. Οι υπολογισμοί είναι:  
* Για την Υψηλή τιμή: $\frac{p_h(t)}{p_{c}(t-1)} - 1$  
* Για την χαμηλή τιμή: $\frac{p_l(t)}{p_{c}(t-1)} - 1$ Αυτή η διαδικασία μηχανικής χαρακτηριστικών αποδίδει συνολικά **8 χαρακτηριστικά** για κάθε χρονικό βήμα, τα οποία αποτελούν το τελικό σύνολο εισόδου για τα μοντέλα νευρωνικών δικτύων του πράκτορα DRL.
### 4.2.3 Βήματα Προεπεξεργασίας Δεδομένων  
Αυτή η ενότητα περιγράφει τα επιπλέον βήματα προεπεξεργασίας δεδομένων που εφαρμόζονται στα ακατέργαστα δεδομένα OHLCV και στα παραγόμενα μηχανικά χαρακτηριστικά πριν εισαχθούν στα μοντέλα Βαθιάς Ενίσχυσης Μάθησης. Αυτά τα βήματα είναι κρίσιμα για την προετοιμασία των δεδομένων σε μια μορφή κατάλληλη για την εκπαίδευση νευρωνικών δικτύων και μπορούν να επηρεάσουν την απόδοση και τη σταθερότητα του μοντέλου.  
**Απουσίες Τιμών:** Με βάση τις διαθέσιμες πληροφορίες σχετικά με το σύνολο δεδομένων και τη διαδικασία μηχανικής χαρακτηριστικών, δεν αναφέρθηκαν συγκεκριμένα μέθοδοι καθαρισμού δεδομένων ή αποκατάστασης για την αντιμετώπιση των ελλειπόντων τιμών στα ακατέργαστα δεδομένα OHLCV ή στα τελικά 8 μηχανικά χαρακτηριστικά. Επομένως, υποτίθεται ότι είτε το σύνολο δεδομένων ήταν σχετικά πλήρες για την επιλεγμένη χρονική περίοδο και συχνότητα, είτε ότι βασίστηκαν στις προεπιλεγμένες ρυθμίσεις του πλαισίου για την αντιμετώπιση πιθανών κενών κατά τη φόρτωση των δεδομένων.  
**Διαχείριση Ακραίων Τιμών:** Τεχνικές ανίχνευσης και μετριασμού των εξωφρενικών τιμών, όπως η κοπή ακραίων τιμών ή η winsorization, δεν αναφέρθηκαν ρητά ως μέρος της διαδικασίας προεπεξεργασίας στο αρχικό υλικό. Κατά συνέπεια, δεν εφαρμόστηκαν συγκεκριμένες μέθοδοι για την αντιμετώπιση των ακραίων τιμών στα δεδομένα τιμών ή στα μηχανικά χαρακτηριστικά σε αυτήν την υλοποίηση.  
**Κανονικοποίηση/Κλιμάκωση Χαρακτηριστικών:** Για να διασφαλιστεί ότι τα τελικά 8 μηχανικά χαρακτηριστικά είναι σε συγκρίσιμη κλίμακα και να διευκολυνθεί η σταθερή εκπαίδευση του νευρωνικού δικτύου, εφαρμόστηκε μια μέθοδος κανονικοποίησης ή κλιμάκωσης. Η συγκεκριμένη τεχνική που χρησιμοποιήθηκε ήταν η **τυποποίηση Z-score**. Αυτό περιλαμβάνει την αφαίρεση του μέσου όρου κάθε χαρακτηριστικού από το σύνολο εκπαίδευσης και τη διαίρεση με την τυπική του απόκλιση. Αυτή η τυποποίηση εφαρμόζεται τόσο στο εκπαιδευτικό όσο και στο δοκιμαστικό σύνολο (χρησιμοποιώντας τον μέσο όρο και την τυπική απόκλιση από το εκπαιδευτικό σύνολο), με αποτέλεσμα χαρακτηριστικά με μέσο όρο περίπου 0 και τυπική απόκλιση περίπου 1.  
**Έλεγχοι Ποιότητας Δεδομένων:** Ενώ οι εκτενείς διαδικασίες διασφάλισης ποιότητας δεδομένων δεν αναλύθηκαν λεπτομερώς, πραγματοποιήθηκαν τυπικοί έλεγχοι για την ακεραιότητα της φόρτωσης δεδομένων και τους αρχικούς υπολογισμούς χαρακτηριστικών για να διασφαλιστεί ότι το σύνολο δεδομένων επεξεργάστηκε σωστά πριν εισαχθεί στην εκπαιδευτική διαδικασία.  
  
### 4.2.4 Διαχωρισμός Δεδομένων Εκπαίδευσης, Επικύρωσης και Δοκιμής  
  
Αυτή η ενότητα περιγράφει τη μεθοδολογία που χρησιμοποιήθηκε για να διαιρεθεί το πλήρες σύνολο δεδομένων σε διακριτά υποσύνολα για την εκπαίδευση και την αξιολόγηση των πρακτόρων Βαθιάς Ενίσχυσης Μάθησης. Μια σωστή διαχωρισμένη βάση δεδομένων είναι απαραίτητη για να διασφαλιστεί ότι η απόδοση του μοντέλου αξιολογείται σε δεδομένα που δεν έχει συναντήσει κατά τη διάρκεια της εκπαίδευσης, παρέχοντας μια αμερόληπτη αξιολόγηση των ικανοτήτων γενίκευσής του. Το σύνολο δεδομένων χωρίστηκε χρησιμοποιώντας μια μεθοδολογία **χρονολογικού (βάσει χρόνου) διαχωρισμού**.  
Αυτή η προσέγγιση είναι τυπική πρακτική στην πρόβλεψη χρονοσειρών και την αξιολόγηση στρατηγικών συναλλαγών, καθώς προσομοιώνει ρεαλιστικά το σενάριο όπου ένα μοντέλο που έχει εκπαιδευτεί σε παρελθόντα δεδομένα χρησιμοποιείται για να λαμβάνει αποφάσεις σε μελλοντικά, αόρατα δεδομένα. Το **Σετ Εκπαίδευσης** περιλαμβάνει δεδομένα από την αρχή του συνόλου δεδομένων, εκτεινόμενα από **17 Αυγούστου 2017, στις 5 π.μ., έως και 5 Σεπτεμβρίου 2021, στις 11 μ.μ.** Το **Σετ Εκπαίδευσης** περιλαμβάνει δεδομένα από την αρχή του συνόλου δεδομένων, που εκτείνονται από **17 Αυγούστου 2017, στις 5 π.μ., έως και τις 5 Σεπτεμβρίου 2021, στις 11 μ.μ.** Αυτή η περίοδος χρησιμοποιείται για την εκπαίδευση του πράκτορα DRL και του συνόλου δασκάλων.  
Αυτή η περίοδος χρησιμοποιείται για την εκπαίδευση του πράκτορα DRL και του συνόλου δασκάλων.  
Το **Σετ Δοκιμών** αποτελείται από τα υπόλοιπα δεδομένα, ξεκινώντας αμέσως μετά το σετ εκπαίδευσης, από **μετά τις 5 Σεπτεμβρίου 2021, στις 11 μ.μ., έως το τέλος του συνόλου δεδομένων στις 2 Φεβρουαρίου 2022, στις 6 π.μ.** Το **Σετ Δοκιμών** αποτελείται από τα υπόλοιπα δεδομένα, που ξεκινούν αμέσως μετά το σετ εκπαίδευσης, από **μετά τις 5 Σεπτεμβρίου 2021, στις 11 μ.μ., έως το τέλος του συνόλου δεδομένων στις 2 Φεβρουαρίου 2022, στις 6 π.μ.** Η απόδοση του εκπαιδευμένου πράκτορα DRL αξιολογείται αποκλειστικά σε αυτό το σετ δοκιμών για να μετρηθεί η αποτελεσματικότητά του σε αόρατες συνθήκες αγοράς. Η απόδοση του εκπαιδευμένου πράκτορα DRL αξιολογείται αποκλειστικά σε αυτό το σύνολο δοκιμών για να μετρηθεί η αποτελεσματικότητά του σε αθέατες συνθήκες αγοράς.  
Για την τελική αξιολόγηση που αναφέρεται σε αυτή τη διατριβή, ένα ξεχωριστό **σύνολο επικύρωσης** **δεν χρησιμοποιήθηκε ρητά** με τον ίδιο τρόπο όπως μια τυπική διαίρεση επικύρωσης για τη ρύθμιση υπερπαραμέτρων κατά τη διάρκεια των κύριων εκπαιδευτικών διαδικασιών. Η ρύθμιση των υπερπαραμέτρων πραγματοποιήθηκε μέσω προπαρασκευαστικών πειραμάτων όπως περιγράφεται στην Ενότητα 4.1.2, και η τελική αξιολόγηση επικεντρώνεται στα μετρικά απόδοσης που επιτεύχθηκαν στο διακριτό σύνολο δοκιμών.  
Η κύρια **λογική** πίσω από αυτή τη χρονική διαίρεση είναι να παρέχει μια ρεαλιστική εκτίμηση της απόδοσης της στρατηγικής συναλλαγών σε πραγματικά **αόρατα δεδομένα**. Η διατήρηση αυστηρής **χρονικής σειράς** μεταξύ των συνόλων εκπαίδευσης και δοκιμής είναι κρίσιμη για την αποφυγή της προκατάληψης προόδου και για να διασφαλιστεί ότι η αξιολόγηση αντικατοπτρίζει με ακρίβεια την απόδοση του πράκτορα όταν αναπτυχθεί σε ένα ζωντανό σενάριο συναλλαγών. Ενώ δεν έχει σχεδιαστεί ρητά για να καταγράψει συγκεκριμένα ποικιλόμορφα καθεστώτα αγοράς εντός των σταθερών περιόδων εκπαίδευσης/δοκιμής, η πολυετής διάρκεια του συνόλου δεδομένων περιλαμβάνει εγγενώς ποικιλόμορφες συνθήκες αγοράς, όπως περίοδοι ανάπτυξης, πτώσης και σταθεροποίησης, παρέχοντας ένα λογικά ποικιλόμορφο περιβάλλον για αξιολόγηση.  
  
### 4.3.1 Υλοποίηση Vanilla PPO  
  
Αυτή η ενότητα εισάγει τη βασική αναφορά που χρησιμοποιείται για σύγκριση στην πειραματική αξιολόγηση: ο πράκτορας **Vanilla PPO**. Αυτή η βάση αναφοράς αντιπροσωπεύει την τυπική εφαρμογή του αλγορίθμου Proximal Policy Optimization χωρίς καμία μορφή Απόσταξης Γνώσης ή καθοδήγηση από δάσκαλο. **Ορισμός:** Ο Vanilla PPO πράκτορας υλοποιεί τον τυπικό αλγόριθμο PPO, μαθαίνοντας μια πολιτική συναλλαγών αποκλειστικά μέσω άμεσης αλληλεπίδρασης με το προσομοιωμένο χρηματοοικονομικό περιβάλλον και την βελτιστοποίηση της βασικής συνάρτησης στόχου PPO. Δεν ενσωματώνει καμία τεχνική Απόσταξης Γνώσης, όπως η αντιστοίχιση λογιστικών ή η Πιθανοτική Μεταφορά Γνώσης.  
**Ρύθμιση:** Για να διασφαλιστεί μια δίκαιη σύγκριση με τον πράκτορα DRL που εκπαιδεύτηκε με KD, η βασική γραμμή Vanilla PPO χρησιμοποιεί την **ίδια αρχιτεκτονική δικτύου** (όπως περιγράφεται στην Ενότητα 3.4, με βάση το πλαίσιο της Ενότητας 4.1.3 του Moustakidis) και τις **ίδιες βασικές υπερπαραμέτρους PPO** (όπως αναλύεται στην Ενότητα 4.1.2), συμπεριλαμβανομένων του ρυθμού εκμάθησης, του μεγέθους παρτίδας, της παραμέτρου κλιπ και των παραμέτρων GAE.  
**Διαφορά:** Η θεμελιώδης διαφορά έγκειται στον στόχο εκπαίδευσής του. Η εκπαίδευση του Vanilla PPO πράκτορα βασίζεται **αποκλειστικά στη συνάρτηση στόχου PPO ($L_{RL}$)**, συγκεκριμένα στον κομμένο υποκατάστατο στόχο (Eq 3.3), ο οποίος προέρχεται από την εμπειρία του πράκτορα και το σήμα ανταμοιβής του περιβάλλοντος. Δεν περιλαμβάνει καμία βαρύτητα KD απώλειες ($L_{log}$, $L_{pkt}$) ή καθοδήγηση από μοντέλα δασκάλων.  
**Εκπαίδευση:** Η διαδικασία εκπαίδευσης για τη βασική γραμμή Vanilla PPO ακολουθεί τον τυπικό κύκλο εκπαίδευσης DRL: ο πράκτορας αλληλεπιδρά με το περιβάλλον, συλλέγει εμπειρία, την αποθηκεύει στη μνήμη αναπαραγωγής και ενημερώνει τις παραμέτρους του δικτύου του ελαχιστοποιώντας τον στόχο $L_{RL}$ χρησιμοποιώντας δειγματοληπτικές παρτίδες, όπως περιγράφεται στην Ενότητα 4.1.2.  
**Σκοπός:** Η υλοποίηση Vanilla PPO χρησιμεύει ως το θεμελιώδες **benchmark απόδοσης**. Συγκρίνοντας την απόδοση και τη σταθερότητα των πρακτόρων που εκπαιδεύτηκαν με προηγμένες τεχνικές KD με αυτή τη βασική γραμμή, η έρευνα μπορεί να ποσοτικοποιήσει τον συγκεκριμένο αντίκτυπο και τα οφέλη που αποδίδονται στην ένταξη της Απόσταξης Γνώσης στο πλαίσιο DRL για το εμπόριο κρυπτονομισμάτων.


### 4.3.2 Βασικές Γραμμές Αναγνώρισης Γνώσης  
  
Εκτός από τη βασική γραμμή Vanilla PPO (Ενότητα 4.3.1) και τις παραδοσιακές στρατηγικές συναλλαγών (Ενότητα 4.3.2), αυτή η διατριβή περιλαμβάνει υλοποιήσεις απλών μεθόδων Απόσταξης Γνώσης (KD) για να χρησιμεύσουν ως σημεία αναφοράς για την αξιολόγηση της αποτελεσματικότητας των προτεινόμενων προηγμένων τεχνικών KD. Αυτές οι βάσεις αναφοράς αντιπροσωπεύουν πιο τυπικές εφαρμογές της KD, εστιάζοντας κυρίως στην αντιστοίχιση λογιστικών.  
  
**Standard Offline Distillation:** Μία βασική γραμμή αναφοράς βασίζεται στην **Standard Offline Distillation**. Σε αυτό το παράδειγμα, η γνώση μεταφέρεται από ένα ή περισσότερα μοντέλα δασκάλων που είναι πλήρως προεκπαιδευμένα και παραμένουν στατικά κατά τη διάρκεια της εκπαίδευσης του μαθητή. Οι δάσκαλοι εκπαιδεύονται πρώτα χρησιμοποιώντας τον τυπικό αλγόριθμο PPO για έναν επαρκή αριθμό εποχών ώστε να συγκλίνουν στα δεδομένα εκπαίδευσης. Ο μηχανισμός KD περιλαμβάνει κυρίως **αντιστοίχιση λογιστικών πιθανοτήτων** με κλιμάκωση θερμοκρασίας, όπου ο μαθητής εκπαιδεύεται να ταιριάζει τις μαλακές κατανομές πιθανοτήτων της εξόδου του προεκπαιδευμένου δασκάλου (ανατρέξτε στην Ενότητα 2.4.2, συμπεριλαμβανομένου του υπολογισμού των μαλακών πιθανοτήτων χρησιμοποιώντας θερμοκρασία $T$ (Εξ. 3.6, 3.7) και την απώλεια αντιστοίχισης λογιστικών πιθανοτήτων $L_{log}$ (Εξ. 3.8)). Ο στόχος εκπαίδευσης του μαθητή συνδυάζει την τυπική απώλεια DRL ($L_{RL}$) και την απώλεια βαρύτητας αντιστοίχισης λογιστικού ($L_{log}$).  
  
**Τυπική Διαδικασία Αποσταγμάτων Διαδικτύου:** Ένας άλλος βασικός δείκτης χρησιμοποιεί **Τυπική Διαδικασία Αποσταγμάτων Διαδικτύου**. Αυτή η προσέγγιση περιλαμβάνει την **ταυτόχρονη εκπαίδευση** του/των δασκάλου/δασκάλων και του μοντέλου του μαθητή. Σε αντίθεση με τη προηγμένη μέθοδο διαδικτυακής συνάθροισης στο προτεινόμενο πλαίσιο, αυτή η τυπική διαδικτυακή βάση μπορεί να χρησιμοποιεί έναν μόνο δάσκαλο ή μια απλή συνάθροιση χωρίς δυναμική επιλογή. Ο μηχανισμός KD βασίζεται και πάλι κυρίως στην **αντιστοίχιση λογιστικών**, με τον μαθητή να εκπαιδεύεται ώστε να ταιριάζει με τις εξελισσόμενες μαλακές κατανομές πιθανοτήτων των εξόδων του δασκάλου(ων) σε κάθε βήμα εκπαίδευσης (ανατρέξτε στην Ενότητα 2.4.2, Εξ. 3.6-3.8). Η απώλεια του μαθητή περιλαμβάνει τον τυπικό στόχο DRL ($L_{RL}$) και την απώλεια βαρύτητας αντιστοίχισης λογιστικού ($L_{log}$).  
  
**Σκοπός:** Αυτές οι τυπικές υλοποιήσεις KD χρησιμεύουν ως κρίσιμα **ορόσημα** για τις πιο προηγμένες τεχνικές KD που αξιολογούνται στη διατριβή. Συγκρίνοντας τις μετρήσεις απόδοσης και σταθερότητας του πράκτορα DRL που εκπαιδεύτηκε με την προτεινόμενη μέθοδο με αυτές τις τυπικές βάσεις KD, η έρευνα μπορεί να απομονώσει και να ποσοτικοποιήσει τα συγκεκριμένα οφέλη που παρέχονται με την ενσωμάτωση στοιχείων όπως η Πιθανιστική Μεταφορά Γνώσης και η προηγμένη στρατηγική διαδικτυακής σύνθεσης.  
  
**Διάκριση:** Αυτές οι τυπικές βάσεις KD διαφέρουν από τη κύρια "Προτεινόμενη Μέθοδος" (Ενότητα 3.3) σε βασικές πτυχές. Συνήθως λείπει το στοιχείο **Probabilistic Knowledge Transfer (PKT)**, το οποίο αποστάζει τη γνώση από ενδιάμεσα στρώματα. Η στρατηγική τους για το σύνολο, αν υπάρχει, είναι απλούστερη, πιθανώς χωρίς δυναμική επιλογή δασκάλου βάσει απόδοσης. Επιπλέον, η διαδικτυακή τους στρατηγική είναι μια βασική ταυτόχρονη εκπαίδευση χωρίς τη συγκεκριμένη διαχείριση συνόλου και τη συνδυασμένη δομή απώλειας PKT/logit της προτεινόμενης προσέγγισης. Αυτές οι διαφορές επιτρέπουν μια ανάλυση παρόμοια με την αφαίρεση των συνεισφορών των προηγμένων στοιχείων KD.  
  
### 4.4.1 Μετρικές Απόδοσης Συναλλαγών  
  
Η αξιολόγηση της αποτελεσματικότητας των στρατηγικών αλγοριθμικού εμπορίου απαιτεί ένα ολοκληρωμένο σύνολο μετρικών που να καταγράφουν όχι μόνο την συνολική οικονομική αξία που παράγεται αλλά και τον σχετικό κίνδυνο και τα χαρακτηριστικά της εμπορικής συμπεριφοράς. Αυτή η ενότητα περιγράφει τα κύρια μετρικά που χρησιμοποιούνται σε αυτή τη διατριβή για την αξιολόγηση της απόδοσης των πρακτόρων DRL και των βασικών στρατηγικών.  
  
**Κέρδη και Ζημίες (PnL):** **Κέρδη και Ζημίες (PnL)** είναι ένα θεμελιώδες μέτρο που ποσοτικοποιεί την σωρευτική απόδοση μιας στρατηγικής συναλλαγών σε μια καθορισμένη περίοδο. Αναπαριστά το καθαρό κέρδος ή ζημία από όλες τις εκτελεσθείσες συναλλαγές. Ενώ η Ενότητα 2.5.4 εισήγαγε το PnL ως το άθροισμα των αποδόσεων με την πάροδο του χρόνου (Εξ. 1.5), στο πλαίσιο των θέσεων και των αποδόσεων, το PnL μπορεί επίσης να αναπαρασταθεί ως το άθροισμα του προϊόντος του μεγέθους της θέσης που κατέχεται τη στιγμή $t$ ($p_t$) και της απόδοσης που επιτεύχθηκε από τη στιγμή $t$ έως τη στιγμή $t+1$ ($r_{t+1}$):  
$$ \text{PnL} = \sum_{t} p_t \cdot r_{t+1} $$  
Ο **στόχος** του PnL είναι να παρέχει ένα άμεσο μέτρο της κερδοφορίας της στρατηγικής σε απόλυτους όρους.  
  
**Δείκτης Sharpe:** Ο **Δείκτης Sharpe** είναι ένα ευρέως χρησιμοποιούμενο μέτρο για την εκτίμηση της απόδοσης προσαρμοσμένης στον κίνδυνο. Ποσοτικοποιεί την απόδοση που αποκτάται πάνω από το επιτόκιο χωρίς κίνδυνο ανά μονάδα μεταβλητότητας (τυπική απόκλιση των αποδόσεων). Για την ετήσια αναγωγή, η συνήθως χρησιμοποιούμενη φόρμουλα είναι:  
$$ \text{Sharpe Ratio} = \frac{\mu_r}{\sigma_r} \sqrt{N} $$  
όπου $\mu_r$ είναι η μέση απόδοση της στρατηγικής κατά την περίοδο, $\sigma_r$ είναι η τυπική απόκλιση των αποδόσεων της στρατηγικής, και $N$ είναι ο αριθμός των περιόδων ανά έτος (π.χ., 8640 για ωριαία δεδομένα στις αγορές κρυπτονομισμάτων). Ένας υψηλότερος δείκτης Sharpe υποδηλώνει καλύτερη απόδοση για ένα δεδομένο επίπεδο κινδύνου. Ο **στόχος** αυτού του δείκτη, όπως εισήχθη από τον Sharpe (1966), είναι να επιτρέπει τη σύγκριση στρατηγικών με διαφορετικά προφίλ κινδύνου, ευνοώντας αυτές που επιτυγχάνουν υψηλότερες αποδόσεις χωρίς να αυξάνουν αναλογικά την αστάθεια.  
  
**Μέγιστη Υποχώρηση:** **Μέγιστη Υποχώρηση (MDD)** μετρά την μεγαλύτερη πτώση από το υψηλότερο στο χαμηλότερο σημείο της αξίας του χαρτοφυλακίου μιας στρατηγικής συναλλαγών σε μια συγκεκριμένη περίοδο. Υπολογίζεται ως η μέγιστη παρατηρούμενη απώλεια από μια κορυφή στο PnL μέχρι μια επόμενη κοιλάδα πριν επιτευχθεί μια νέα κορυφή:  
$$ \text{MDD} = \max_{s<t} (\text{PnL}_s - \text{PnL}_t) $$ είναι το σωρευτικό $PnL$ σε μια ιστορική κορυφαία χρονική στιγμή $s$, και $\text{PnL}_t$ είναι το σωρευτικό $PnL$ σε μια επόμενη χρονική στιγμή $t$. Ο **στόχος** του MDD είναι να ποσοτικοποιήσει τη μεγαλύτερη ιστορική απώλεια που υπήρξε από τη στρατηγική, λειτουργώντας ως βασικός δείκτης του κινδύνου πτώσης και της πιθανής έκθεσης κεφαλαίου.  
  
**Χρόνος Ανάκαμψης:** **Ο Χρόνος Ανάκαμψης** μετρά τη διάρκεια που χρειάζεται για να ανακάμψει το χαρτοφυλάκιο μιας στρατηγικής συναλλαγών από μια πτώση και να φτάσει σε μια νέα κορυφή κεφαλαίου. Συνήθως υπολογίζεται ως η μέση διάρκεια όλων των πτώσεων που έχουν σημειωθεί κατά την περίοδο αξιολόγησης. Ο **στόχος** του Χρόνου Ανάκαμψης είναι να αξιολογήσει την ανθεκτικότητα της στρατηγικής και την ικανότητά της να ανακάμπτει γρήγορα από τις απώλειες, παρέχοντας πληροφορίες σχετικά με τις πιθανές περιόδους κλειδώματος κεφαλαίων κατά τη διάρκεια των πτωτικών αγορών.  
  
**Μετρικές Συμπεριφοράς Συναλλαγών:** Εκτός από την απόδοση και τον κίνδυνο, οι μετρικές που περιγράφουν τη **συμπεριφορά συναλλαγών** του πράκτορα παρέχουν πολύτιμες πληροφορίες σχετικά με τα χαρακτηριστικά της μαθημένης πολιτικής. Αυτές οι μετρήσεις βοηθούν στην κατανόηση του *πώς* ο πράκτορας επιτυγχάνει τα αποτελέσματά του. Συγκεκριμένα μετρικά συμπεριφοράς συναλλαγών περιλαμβάνουν τον **Αναλογία Μακροχρόνιων/Βραχυχρόνιων/Ουδέτερων Θέσεων** (η αναλογία του χρόνου που δαπανάται σε κάθε θέση), τον **Αριθμό Αλλαγών Θέσεων** (συχνότητα των αγορών/πωλήσεων) και τη **Μέση Διάρκεια Θέσης** (πόσο καιρό κρατούνται συνήθως οι θέσεις). Ο **συλλογικός σκοπός** αυτών των μετρικών είναι να προσφέρουν ερμηνεία στη στρατηγική του πράκτορα, επιτρέποντας την ανάλυση πέρα από τους τελικούς αριθμούς απόδοσης και διευκολύνοντας τη σύγκριση των στυλ συναλλαγών μεταξύ διαφορετικών μοντέλων.

[VISUALIZATION PLACEHOLDER: Type=Table, Title='Summary of Evaluation Metrics', Data='Columns: Metric, Definition, Purpose. Rows: PnL, Sharpe Ratio, Maximum Drawdown, Recovery Time, Trading Behavior Metrics.']
[VISUALIZATION PLACEHOLDER: Type=Table, Title='Example Trading Performance Results', Data='Columns: Strategy (Vanilla PPO, etc.), PnL (Mean ± Std), Sharpe Ratio (Mean ± Std), MDD (Mean ± Std), Recovery Time (Mean). Rows for each evaluated strategy.']
[VISUALIZATION PLACEHOLDER: Type=Table, Title='Example Trading Behavior Results', Data='Columns: Strategy, Long Ratio, Short Ratio, Neutral Ratio, Position Changes (per period), Avg Position Duration. Rows for each evaluated strategy.']


### 4.4.2 Μετρικές Σταθερότητας Εκπαίδευσης  
  
Πέρα από την αξιολόγηση της τελικής εμπορικής απόδοσης, είναι κρίσιμο να αξιολογηθεί η συνέπεια και η αξιοπιστία της διαδικασίας εκπαίδευσης καθαυτής. **Μετρικές σταθερότητας εκπαίδευσης** ποσοτικοποιούν πόσο αναπαραγώγιμα είναι τα αποτελέσματα σε πολλές ανεξάρτητες εκτελέσεις εκπαίδευσης, βοηθώντας να προσδιοριστεί αν η μαθημένη πολιτική είναι ανθεκτική ή πολύ ευαίσθητη σε παράγοντες όπως η τυχαία αρχικοποίηση. Αυτή η ενότητα περιγράφει τα κύρια μετρικά που χρησιμοποιούνται για την αξιολόγηση της σταθερότητας εκπαίδευσης.  
  
**Συντελεστής Μεταβλητότητας (CV):** Ο **Συντελεστής Μεταβλητότητας (CV)** χρησιμοποιείται για να μετρήσει τη σχετική μεταβλητότητα της τελικής απόδοσης συναλλαγών σε πολλές εκπαιδευτικές εκτελέσεις. Ορίζεται ως ο λόγος της τυπικής απόκλισης των τελικών τιμών PnL σε πολλές εκτελέσεις προς την απόλυτη τιμή του μέσου όρου των τελικών τιμών PnL:  
$$ \text{CV} = \frac{\sigma_{\text{PnL}}}{\left|\mu_{\text{PnL}}\right|} $$ 
όπου $\sigma_{\text{PnL}}$ είναι η τυπική απόκλιση του τελικού σωρευτικού PnL σε διάφορες εκτελέσεις, και $\mu_{\text{PnL}}$ είναι ο μέσος όρος του τελικού σωρευτικού PnL σε διάφορες εκτελέσεις. Ένας χαμηλότερος CV υποδηλώνει λιγότερη μεταβλητότητα σε σχέση με την μέση απόδοση, υποδεικνύοντας υψηλότερη σταθερότητα εκπαίδευσης. Ο **στόχος** του CV είναι να παρέχει ένα τυποποιημένο μέτρο διασποράς που επιτρέπει τη σύγκριση της σταθερότητας ακόμη και όταν οι στρατηγικές έχουν διαφορετικές μέσες τιμές PnL. Ένα σχέδιο ταξινόμησης σταθερότητας μπορεί να εφαρμοστεί με βάση τις τιμές CV (π.χ., Υψηλή Σταθερότητα για CV < 0.1, Μέτρια Σταθερότητα για 0.1 <= CV < 0.5, Χαμηλή Σταθερότητα για CV >= 0.5), όπως χρησιμοποιείται σε σχετική εργασία.  
  
**Διαστήματα Εμπιστοσύνης:** **Διαστήματα Εμπιστοσύνης (CIs)** παρέχουν μια σειρά τιμών εντός των οποίων είναι πιθανό να πέσει ο πραγματικός μέσος όρος ενός μετρικού (π.χ., μέσος τελικός PnL), με ένα καθορισμένο επίπεδο εμπιστοσύνης (π.χ., 95%). Για τον μέσο PnL σε $n$ εκπαιδευτικές δοκιμές, το διάστημα εμπιστοσύνης 95% υπολογίζεται ως εξής:  
 $$ \text{CI}_{95\%} = \mu_{\text{PnL}} \pm t_{\alpha/2, n-1} \cdot \frac{\sigma_{\text{PnL}}}{\sqrt{n}} $$
όπου $t_{\alpha/2, n-1}$ είναι η κρίσιμη τιμή από την κατανομή t για επίπεδο εμπιστοσύνης 95% και $n-1$ βαθμούς ελευθερίας. Ο **στόχος** των 95% CI είναι να παρέχουν ένα μέτρο της στατιστικής σημασίας της μέσης απόδοσης, υποδεικνύοντας την αξιοπιστία της εκτιμώμενης μέσης PnL. Τα στενότερα ΔΔ υποδηλώνουν μεγαλύτερη εμπιστοσύνη στο μέσο αποτέλεσμα και, επομένως, μεγαλύτερη σταθερότητα.  
  
**Ανάλυση Σύγκλισης:** **Η ανάλυση σύγκλισης** αξιολογεί πόσο σταθερά και ομαλά σταθεροποιείται η διαδικασία εκπαίδευσης με την πάροδο του χρόνου. Μία μέθοδος περιλαμβάνει την εξέταση της μεταβλητότητας ενός μετρικού απόδοσης (π.χ. PnL) στις τελικές φάσεις της εκπαίδευσης. Ένα απλό **κριτήριο σύγκλισης** μπορεί να οριστεί, για παράδειγμα, ελέγχοντας αν ο Συντελεστής Μεταβλητότητας του μετρικού κατά την τελική φάση της περιόδου εκπαίδευσης (π.χ., τις τελευταίες 20 εποχές) πέφτει κάτω από ένα συγκεκριμένο όριο:  
$$ \frac{\sigma_{\text{τελευταίοι 20}}}{\left|\mu_{\text{τελευταίοι 20}}\right|} < 0.01 $$  
όπου $\sigma_{\text{final 20}}$ και $\mu_{\text{final 20}}$ είναι η τυπική απόκλιση και ο μέσος όρος του μετρικού κατά τις τελευταίες 20 εποχές, αντίστοιχα. Ο **στόχος** της ανάλυσης σύγκλισης είναι να αξιολογήσει εάν η διαδικασία εκπαίδευσης φτάνει σε μια σταθερή κατάσταση και πόση μεταβλητότητα υπάρχει στην απόδοση της τελικής πολιτικής κατά τη διάρκεια αυτής της σταθερής φάσης.

[VISUALIZATION PLACEHOLDER: Type=Table, Title='Training Stability Metrics Summary', Data='Columns: Metric, Definition, Purpose, Formula. Rows: Coefficient of Variation, Confidence Intervals, Convergence Analysis Criterion.']
[VISUALIZATION PLACEHOLDER: Type=Table, Title='Example Stability Results', Data='Columns: Strategy, Mean Final PnL, Std Dev Final PnL, CV, 95% CI (lower, upper), Convergence Status (Met/Not Met). Rows for each evaluated strategy.']


### 5.1.1 Overall Performance Comparison

Τα κύρια ευρήματα από την πειραματική αξιολόγηση των διαφόρων τεχνικών DRL και Εξαγωγής Γνώσης (KD) που χρησιμοποιούνται σε αυτή τη διατριβή συνοψίζονται σε αυτή την ενότητα. Για να δείξουμε τις ομοιότητες, τις διαφορές και την αποτελεσματικότητα των υλοποιήσεων, τα αποτελέσματα συγκρίνονται με τα αντίστοιχα ευρήματα  στην αρχική πτυχιακή.  
  
Μαζί με τα ευρήματα που παρουσιάζονται στη διατριβή του Μουστακίδη, ο μέσος σωρευτικός Κέρδος και Ζημία (PnL) και η τυπική του απόκλιση σε πολλές ανεξάρτητες εκπαιδευτικές δοκιμές για κάθε αξιολογούμενη μέθοδο συγκρίνονται άμεσα στον ακόλουθο πίνακα:

| Method               | Our Results (%) | Thesis Results (%) | Difference | % Difference |     |
| :------------------- | :-------------- | :----------------- | :--------- | :----------- | --- |
| Baseline             | 16.98 ± 1.45    | 14.81 ± 0.92       | -2.17      | -14.68%      |     |
| Offline Distillation | 17.36 ± 1.39    | 16.96 ± 0.31       | -0.40      | -2.35%       |     |
| Online Distillation  | 16.84 ± 1.93    | 17.84 ± 0.33       | +1.00      | +5.61%       |     |
| Online PKT           | 16.43 ± 2.30    | 18.56 ± 0.06       | +2.13      | +11.46%      |     |
| Proposed Method      | 16.94 ± 1.40    | 18.66 ± 0.02       | +1.72      | +9.21%       |     |


![[thesis_comparison.png]]
Ακολουθεί μια σύνοψη της απόδοσης κάθε μεθόδου στις δοκιμές μας σε σύγκριση με τα ευρήματα της διατριβής του Μουστακίδη:  
  
* **Baseline:** Επιτύχαμε έναν μέσο PnL της τάξης του **16.98%** με τυπική απόκλιση **1.45%** χρησιμοποιώντας τη βασική γραμμή Vanilla PPO. Αν και η τυπική μας απόκλιση είναι επίσης υψηλότερη, αυτό το αποτέλεσμα είναι αισθητά υψηλότερο από το 14,81% μέσο PnL για τη βασική τους γραμμή, γεγονός που μπορεί να υποδηλώνει διακυμάνσεις στην συγκεκριμένη πειραματική ρύθμιση ή διαχωρισμό δεδομένων.  
* **Offline Distillation:** Με μέσο PnL **17.36%** και τυπική απόκλιση **1.39%**, αυτή η προσέγγιση παρήγαγε το υψηλότερο μέσο PnL από όλες τις υλοποιήσεις μας. Με μια διαφορά μόλις -2,35%, το μέσο PnL του είναι το πιο κοντινό στην αναφερόμενη τιμή της διατριβής (16,96%), υποδεικνύοντας την αποτελεσματικότητα της μεταφοράς γνώσης εκτός σύνδεσης στην διαμόρφωσή μας. Η τυπική μας απόκλιση είναι πολύ μεγαλύτερη από το 0,31% που αναφέρεται στη διατριβή.  
* **Online Distillation:** Ο μέσος όρος PnL του **16,84%** (τυπική απόκλιση **1,93%**) επιτεύχθηκε από την υλοποίηση της Διαδικτυακής Απόσταξης μας. Αυτό υποδηλώνει μεγαλύτερη μεταβλητότητα μεταξύ των εκτελέσεων και είναι λιγότερο από το 17,84% που αναφέρεται στη διατριβή του Μουστακίδη.  
* **PKT Online:** Ένα μέσο PnL της τάξης του **16,43%** (τυπική απόκλιση **2,30%**) προήλθε από την υλοποίηση του Online PKT. Αυτό το PnL είναι σημαντικά χαμηλότερο από το 18,56% που αναφέρεται στη διατριβή και χαμηλότερο από τη βάση μας. Επιπλέον, έχει την υψηλότερη τυπική απόκλιση από οποιαδήποτε μέθοδο, γεγονός που μπορεί να υποδηλώνει αστάθεια σε αυτήν την συγκεκριμένη υλοποίηση.  
* **Ensemble PKT Online:** Ένας μέσος PnL της τάξης του **16,94%** (τυπική απόκλιση **1,40%**) επιτεύχθηκε με την εφαρμογή της Προτεινόμενης Μεθόδου (Online Ensemble με PKT). Αν και είναι κάτω από το 18,66% που αναφέρεται στη διατριβή του Μουστακίδη για την ίδια μέθοδο, αυτή η απόδοση είναι εξαιρετικά κοντά στη βάση μας. Επιπλέον, η τυπική μας απόκλιση είναι σημαντικά υψηλότερη από το 0,02%.

Seed Variation for Baseline

![[seed_variation.png]]


Seed Variation for Offline Distillation

![[seed_variation 1.png]]
Seed Variation for Online Distillation
![[seed_variation 2.png]]

Seed Variation for Online PKT Distillation
![[seed_variation 3.png]]
## PNL Performance Comparison

  
### Long-Term Performance  
  
Η εξέταση του PnL μας με την πάροδο του χρόνου αποκαλύπτει ότι: 1. Κατά την πρώτη περίοδο διαπραγμάτευσης, όλες οι μέθοδοι εμφανίζουν συγκρίσιμα πρότυπα απόδοσης.  
2. Σε μακροχρόνιες περιόδους, οι διαφορές μεταξύ των μεθόδων γίνονται πιο εμφανείς.  
3. Η τεχνική απόσταξης εκτός σύνδεσης υπερβαίνει σταθερά τη βασική γραμμή.  
  
### Ανάλυση Σταθερότητας  
  
Η τυπική απόκλιση διαφόρων σπόρων προσφέρει πληροφορίες σχετικά με το πόσο σταθερή είναι κάθε στρατηγική:  
1. Οι τυπικές αποκλίσεις σε όλες τις υλοποιήσεις μας είναι υψηλότερες από αυτές που αναφέρονται στη διατριβή.  
2. Από τις υλοποιήσεις μας για απόσταξη, η μέθοδος απόσταξης εκτός σύνδεσης παρουσιάζει τη χαμηλότερη τυπική απόκλιση.  
3. Μεταξύ των σπόρων, η διαδικτυακή μέθοδος PKT εμφανίζει τη μεγαλύτερη μεταβλητότητα.  
  
### Αξιολόγηση σε Σχέση με τα Αρχικά αποτελέσματα  
  
Παρά κάποιες διαφορές μεταξύ των ευρημάτων μας και αυτών που παρουσιάζονται στη διατριβή, μπορούν να σημειωθούν τα εξής σημαντικά σημεία:  
  
* Κατάταξη αρχικών αποτελεσμάτων: Proposed > Online PKT > Online Distillation > Offline Distillation > Baseline **Different Performance Rankings** ** 
- Η κατάταξή μας  Online Distillation > Online PKT > Offline Distillation > Proposed/Baseline
  
2. **Διαφορές στις Τυπικές Αποκλίσεις**: - Οι υλοποιήσεις μας εμφανίζουν συνεχώς υψηλότερες τυπικές αποκλίσεις  
Πολύ χαμηλές τυπικές αποκλίσεις για προηγμένες τεχνικές αναφέρονται στη διατριβή (π.χ., 0,02% για την προτεινόμενη μέθοδο).  
  
3. **Σχετικές Βελτιώσεις**: Τα ευρήματά μας και η διατριβή δείχνουν ότι οι τεχνικές απόσταξης συνήθως υπερτερούν της βασικής γραμμής.  
Ο βαθμός βελτίωσης διαφέρει, ειδικά για πιο προηγμένες τεχνικές.  
  
## Ανάλυση και Συζήτηση  
  
### Πιθανές Αιτίες των Ανισοτήτων  
  
*  **Λεπτομέρειες Υλοποίησης**: Οι διαφορές στην απόδοση μπορεί να εξηγηθούν από μικρές προσαρμογές στις λεπτομέρειες υλοποίησης, όπως οι ρυθμίσεις του βελτιστοποιητή, η αρχικοποίηση του δικτύου ή η ρύθμιση υπερπαραμέτρων.  
- Αυτές οι λεπτομέρειες έχουν σημαντική επίδραση στο πόσο καλά λειτουργούν οι τεχνικές απόσταξης γνώσης.  
  
*  **Προεπεξεργασία Δεδομένων**: - Η απόδοση του μοντέλου μπορεί να επηρεαστεί από λεπτές παραλλαγές στην μηχανική χαρακτηριστικών ή στην κανονικοποίηση δεδομένων.  
- Εξαιτίας της ακραίας μεταβλητότητας της αγοράς κρυπτονομισμάτων, ακόμη και μικρές παραλλαγές στη διαχείριση των δεδομένων μπορούν να έχουν διαφορετικά αποτελέσματα.  
  
Η σύγκλιση του μοντέλου μπορεί να επηρεαστεί από παραλλαγές στο υλικό, τις εκδόσεις λογισμικού ή τους υπολογιστικούς πόρους στο περιβάλλον εκπαίδευσης.  
Οι επιδράσεις της τυχαίας αρχικοποίησης του seed μπορεί να διαφέρουν ανάλογα με το περιβάλλον.  
  
### Σημαντικά Συμπεράσματα  
  
**Αποτελεσματικότητα της Απόσταξης Γνώσης**: Τα πειράματά μας επιβεβαιώνουν ότι η απόσταξη γνώσης μπορεί να βελτιώσει την απόδοση του trading πάνω από τη βασική γραμμή, ακόμη και σε περιπτώσεις όπου αποκλίνει από τα ευρήματα της διατριβής.  
Από όλες τις υλοποιήσεις μας, η μέθοδος απόσταξης εκτός σύνδεσης παρουσιάζει τη πιο συνεπή βελτίωση. Από όλες τις υλοποιήσεις μας, η μέθοδος αποσταγμάτων εκτός σύνδεσης παρουσιάζει τη πιο συνεπή βελτίωση.  
  
2. **Συμβιβασμός Σταθερότητας-Απόδοσης**: - Φαίνεται ότι η σταθερότητα και η απόδοση συμβιβάζονται.  
- Φαίνεται ότι η σταθερότητα και η απόδοση ανταγωνίζονται. Αν και οι πιο εξελιγμένες τεχνικές μπορεί να παράγουν καλύτερα αποτελέσματα, μπορεί επίσης να αυξήσουν τη μεταβλητότητα.  
  
3. **Ευαισθησία Υλοποίησης**: - Η απόσταξη γνώσης και η ενισχυτική μάθηση για το χρηματοοικονομικό εμπόριο είναι εξαιρετικά ευαίσθητες στις λεπτομέρειες της υλοποίησης.  
Για αξιόπιστα αποτελέσματα, η προσεκτική ρύθμιση και η επικύρωση σε διάφορους σπόρους είναι απαραίτητες.  
  
## Συμπέρασμα  
  
Οι δοκιμές μας επιβεβαίωσαν την αποτελεσματικότητα της απόσταξης γνώσης στη βελτίωση της ενισχυτικής μάθησης για το χρηματοοικονομικό εμπόριο και αναπαρήγαγαν με επιτυχία τη γενική μεθοδολογία που περιγράφεται στη διατριβή. Το γενικό μοτίβο υποδεικνύει ότι οι τεχνικές απόσταξης μπορούν να βελτιώσουν την απόδοση των συναλλαγών, παρά τις διακυμάνσεις σε συγκεκριμένους δείκτες απόδοσης.  
  
Στην υλοποίησή μας, η μέθοδος απόσταξης εκτός σύνδεσης παρήγαγε τα πιο συνεπή αποτελέσματα, με PnL 17,36% σε σύγκριση με το 16,98% της βασικής γραμμής. Η υλοποίησή μας μπορεί να χρειάζεται περαιτέρω βελτίωση για τις πιο προηγμένες τεχνικές (Online Distillation, Online PKT και Proposed) προκειμένου να επιτύχουμε τα επίπεδα απόδοσης που αναφέρονται στη διατριβή.  
  
Αυτά τα αποτελέσματα δείχνουν πόσο ευαίσθητες είναι οι τεχνικές ενισχυτικής μάθησης στις λεπτομέρειες της υλοποίησης και πόσο κρίσιμη είναι η προσεκτική ρύθμιση των υπερπαραμέτρων και ο πειραματικός έλεγχος. Τα ευρήματα προσφέρουν μια ισχυρή βάση για περαιτέρω έρευνα και βελτίωση των τεχνικών απόσταξης γνώσης για χρηματοοικονομικό trading.

![[pnl_over_baseline.png]]

![[pnl_over_time.png]]
![[stability_comparison.png]]

[VISUALIZATION PLACEHOLDER: Type=Table, Title='Overall Performance Comparison Table', Data='Columns: Method, Our Results (Mean PnL ± Std Dev), Thesis Results (Mean PnL ± Std Dev), Difference (Our - Thesis PnL), % Difference (Our vs Thesis PnL).']
[VISUALIZATION PLACEHOLDER: Type=BarChart, Title='Mean PnL Comparison: Our Results vs Thesis Results', Data='Categories: Methods. Bars: Our Mean PnL, Thesis Mean PnL. Include error bars for Std Dev.']