### 2.4.2.A Transformed Teacher Matching (TTM) and Weighted TTM (WTTM)

This section discusses **Transformed Teacher Matching (TTM)** and **Weighted TTM (WTTM)**, which are advanced Knowledge Distillation techniques that propose modifications to the traditional application of temperature scaling in logit-based distillation. These methods stem from questioning the necessity of applying temperature scaling to both teacher and student logits.

**Transformed Teacher Matching (TTM):** TTM introduces a core modification by dropping temperature scaling entirely on the student side. In this approach, the teacher-side temperature scaling (with temperature $T$) is reinterpreted as applying a power transform (with exponent $\gamma = 1/T$) directly to the teacher's output probability distribution. The student is then trained to match this power-transformed teacher distribution. A key theoretical insight of TTM is that its objective function (referred to as $L_{TTM}$ in the TTM paper, Eq 2) inherently includes a **Rényi entropy term**. This Rényi entropy term acts as an additional regularizer compared to standard KD, promoting smoother, less confident student policies. The claimed benefit of TTM is **improved generalization** compared to standard KD, attributed to this inherent regularization.

**Weighted TTM (WTTM):** WTTM is introduced as an extension of TTM designed for further performance enhancement. It introduces a **sample-adaptive weighting coefficient** into the TTM objective. The **weighting mechanism** is based on quantifying the smoothness of the teacher's power-transformed probability distribution for each individual sample, typically using a power sum $U_\gamma(p)$. The intuition behind this weighting is to make the student pay more attention to matching the teacher's outputs for samples where the teacher is less confident or the distribution is smoother, as these might contain richer "dark knowledge." The claimed benefit of WTTM is **further improvement** over TTM, achieving state-of-the-art accuracy in certain domains by leveraging this sample-adaptive focus.

In summary, both TTM and WTTM primarily focus on distilling knowledge from the teacher's output distributions (logits/action probabilities). However, they aim to achieve better generalization and performance compared to standard logit matching through the theoretical grounding in power transforms and Rényi entropy (TTM) and the introduction of sample-adaptive weighting based on teacher output smoothness (WTTM).

## Chapter 3: Proposed Method
This chapter presents the specific methodology developed in this thesis to address the research problem of enhancing the stability and performance of Deep Reinforcement Learning (DRL) agents for cryptocurrency trading. Building upon the theoretical background and related work discussed in Chapter 2, this chapter details the architecture and components of the proposed DRL framework, focusing on the foundational DRL algorithm and the novel integration of advanced Knowledge Distillation (KD) techniques.

The proposed methodology integrates several distinct KD strategies with a DRL agent built upon the **Proximal Policy Optimization (PPO)** algorithm, which serves as the foundational DRL component. The KD techniques implemented and integrated include **logit-based distillation methods**, specifically **Transformed Teacher Matching (TTM)** and **Weighted TTM (WTTM)**, and a **feature-based distillation method**, **Probabilistic Knowledge Transfer (PKT)**. These KD methods are employed within an **online teacher ensemble framework**, where knowledge is transferred dynamically from a set of teacher models to the student agent. 

The overall aim of this integrated approach is to create a more robust and effective DRL agent by synergistically combining these techniques to mitigate training instability and improve trading performance in volatile cryptocurrency markets. 

The revised structure of Chapter 3 is as follows: Section 3.1 presents the core PPO-based Reinforcement Learning Framework. Section 3.2 details the specific Knowledge Distillation Strategies Employed, covering both TTM/WTTM and PKT. Section 3.3 describes the Online Ensemble mechanism and the final Combined Loss formulation. Finally, Section 3.4 details the Network Architecture used for the DRL agents.

#### 3.2.1 Overview of Distillation in the Framework

This section provides an overview of the Knowledge Distillation (KD) strategies incorporated into the proposed methodology. These techniques are fundamental to the framework's approach to enhancing the Deep Reinforcement Learning (DRL) agent's training stability and performance by transferring learned knowledge from teacher models.

The proposed methodology integrates several distinct KD strategies, each designed to transfer different types of knowledge from the teacher models to the student DRL agent. These approaches can be broadly categorized into two main types:

*   **Logit-Based Distillation:** This category includes methods that focus on transferring knowledge encoded in the teacher's final output layer, specifically its action probability distributions or logits. Within this framework, advanced techniques such as **Transformed Teacher Matching (TTM)** and **Weighted TTM (WTTM)** are employed to refine the traditional logit matching process and encourage beneficial student policy properties like smoother distributions.
*   **Feature-Based Distillation:** This category encompasses methods that focus on transferring knowledge embedded in the teacher's internal representations, specifically its intermediate feature vectors. **Probabilistic Knowledge Transfer (PKT)** is the primary technique used here, aiming to transfer the structural relationships and geometry of the teacher's learned feature space.

These diverse distillation approaches are implemented within an **online ensemble framework**, where knowledge is transferred dynamically from a set of teacher models to the student during simultaneous training. The overall goal of combining these logit-based and feature-based distillation strategies within an online ensemble is to provide robust, multifaceted, and adaptive guidance to the student agent, thereby mitigating the challenges of training DRL in volatile financial markets and improving the reliability and profitability of the learned trading policy.

#### 3.2.3 Feature-Based Distillation: PKT

This section details the implementation of **Probabilistic Knowledge Transfer (PKT)**, the feature-based distillation method employed in this thesis. PKT, as introduced in Chapter 2 and implemented based on the methodology described in the Moustakidis thesis (drawing from Passalis & Tefas's work), focuses on transferring knowledge embedded in the teacher's internal representations.

**Core Idea:** The core principle of PKT is to match the probability distributions of data samples in the feature space of a chosen intermediate layer in the teacher and student networks. This process aims to transfer the geometric structure and relationships learned by the teacher's internal representations to the student, providing a different form of guidance than output-level distillation.

**Implementation Details:** The implementation of PKT in this thesis follows the specifics outlined in the Moustakidis source thesis:
*   **Layer Selection:** As specified in Moustakidis Section 4.2.1, the intermediate layer used for feature extraction is the **linear layer located after the LSTM layer and before the actor and critic heads** (referencing Figure 4.2 of Moustakidis). Feature vectors are extracted from the output of this layer for both teacher(s) and the student.
*   **Similarity Calculation:** The similarity between pairs of these feature vectors is calculated using the **Cosine Similarity kernel**, as confirmed in Moustakidis Section 4.2.1 and defined in Moustakidis Equation 4.3. This kernel is used to compute the conditional probability distributions $p_{i|j}$ for the teacher and $q_{i|j}$ for the student, representing pairwise affinities in the feature space (Moustakidis Eq 3.9, 3.10).
*   **Loss Function:** The **PKT loss function**, denoted as $L_{pkt}$, quantifies the divergence between these feature-space probability distributions. It is defined as the KL divergence between $p_{i|j}$ and $q_{i|j}$, calculated over a batch of data (Moustakidis Eq 3.11). Minimizing $L_{pkt}$ encourages the student's feature space to align with the teacher's.

**Integration:** This $L_{pkt}$ is the **feature-based distillation term** in the overall combined loss function used for training the student DRL agent (as outlined in Section 3.3). It is combined with the PPO loss and the logit-based distillation loss(es), weighted by a factor such as $\beta_2$ or `feature_distillation_weight` (as per Moustakidis Eq 3.14). This integration ensures that the student learns not only from environmental rewards and output-level guidance but also from the structural knowledge embedded in the teacher's intermediate representations.

#### 3.3 Online Ensemble Distillation and Combined Loss
This section details how the individual Knowledge Distillation (KD) strategies—logit-based (TTM, WTTM) and feature-based (PKT)—are integrated within an online ensemble framework and combined to form the student agent's training objective, drawing from the methodology in the Moustakidis source thesis (Section 3.3). 
**Online Ensemble Mechanism:** The proposed framework utilizes an **online ensemble mechanism**. This involves training multiple teacher models alongside the student agent. The teachers in the ensemble are identical in architecture and hyperparameters to the student but differ only by their initial random seed initialization. Training is conducted **simultaneously** for both the teacher ensemble and the student, with both receiving the same data batches from the environment during each training iteration. As described in Moustakidis Section 3.3, a **two-phase teacher selection process** is employed: initially, all teacher models in the ensemble are trained for a set number of epochs (e.g., 50 epochs). Following this initial phase, the best $N$ performing teachers are selected based on a performance metric such as their cumulative training PnL. 
The simultaneous online training then continues with only this selected ensemble of $N$ teachers and the student. This dynamic selection ensures that the student is guided by the most effective teacher policies learned during the initial training period. 
**Knowledge Aggregation from Ensemble:** Knowledge transferred from the selected teacher ensemble is aggregated by averaging the distillation losses computed with respect to each teacher. For **logit-based distillation** (TTM or WTTM), the respective loss term (e.g., $D(p_T^t || q)$ for TTM) is calculated between the student and each of the $N$ active teachers and then averaged to produce the ensemble logit-based distillation loss, $L_{log\_effective\_ensemble}$. 
Similarly, for **PKT**, the $L_{pkt}$ (Moustakidis Eq 3.11) is calculated between the student and each of the $N$ active teachers and then averaged to produce the ensemble PKT loss, $L_{pkt\_ensemble}$. This averaging principle follows that outlined in Moustakidis Equation 3.13 for aggregating distillation losses from an ensemble. 
**Final Combined Loss Function:** The student agent's parameters are updated by minimizing a **final combined loss function** that integrates the standard DRL loss with the ensemble-averaged distillation losses. Adapting Moustakidis Equation 3.14, the overall training objective is defined as: $$ L = L_{RL} + \beta_1 \cdot L_{log\_effective\_ensemble} + \beta_2 \cdot L_{pkt\_ensemble} $$ Here, $L_{RL}$ is the PPO clipped surrogate objective (Eq 3.3), driving learning from environmental rewards. $L_{log\_effective\_ensemble}$ is the ensemble-averaged logit-based distillation loss (either TTM or WTTM, depending on the configuration), and $L_{pkt\_ensemble}$ is the ensemble-averaged PKT loss (Moustakidis Eq 3.11). $\beta_1$ and $\beta_2$ are weighting factors that control the relative contribution of the logit-based and feature-based distillation terms, respectively. The specific values of $\beta_1$ and $\beta_2$ are hyperparameters determined during experimentation. The different `distillation_mode` settings in the implementation control which of these distillation components are active in the final loss calculation, allowing for evaluation of individual KD types and their combinations.
## Chapter 4: Experimental Evaluation
This chapter focuses on the experimental design and setup developed to empirically evaluate the effectiveness of various Knowledge Distillation (KD) strategies integrated within a Deep Reinforcement Learning (DRL) framework for automated financial trading. The experiments are designed to provide quantitative evidence regarding the impact of different KD approaches on the performance and stability of DRL agents in volatile cryptocurrency markets. 
Specifically, this chapter will detail the setup for comparing distinct types of knowledge transfer. This includes evaluating **logit-based distillation methods**, such as Transformed Teacher Matching (TTM) and Weighted TTM (WTTM), against **feature-space distillation methods**, particularly Probabilistic Knowledge Transfer (PKT). The main goal of this empirical assessment is to determine how effectively these different KD approaches enhance DRL agent performance, measured by metrics like Profit and Loss (PnL) and risk-adjusted returns, and improve training stability in the challenging environment of cryptocurrency markets. 
The structure of this chapter is outlined as follows: Section 4.1 describes the detailed Experimental Setup, covering the configuration of the DRL agent, KD implementations, and the trading environment. Section 4.2 provides a thorough Dataset Description, detailing the financial data utilized and the feature engineering process. Section 4.3 defines the Baseline Methods used for comparative analysis. Section 4.4 presents the comprehensive suite of Evaluation Metrics employed to quantify performance and stability. Finally, Section 4.5 discusses Specific Implementation Details related to the software and technical configuration.